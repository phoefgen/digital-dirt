{"archive":[{"service_name":"AWS Management Console","summary":"[RESOLVED] Increased Error Rates","date":"1476414369","status":1,"details":"","description":"<div><span class=\"yellowfg\"> 8:07 PM PDT</span>&nbsp;We are investigating elevated error rates for the AWS Management Console in the AP-NORTHEAST-1 Region.</div><div><span class=\"yellowfg\"> 9:11 PM PDT</span>&nbsp;Between 7:16 PM and 8:52 PM PDT, some customers experienced increased errors with the AWS Management Console in the AP-NORTHEAST-1 Region. This issue has been resolved and the service is operating normally. </div>","service":"management-console"},{"service_name":"Amazon Redshift (Tokyo)","summary":"[RESOLVED] Increased API Error Rates","date":"1476417847","status":1,"details":"","description":"<div><span class=\"yellowfg\"> 8:12 PM PDT</span>&nbsp;We are investigating increased API error rates in the AP-NORTHEAST-1 Region.</div><div><span class=\"yellowfg\"> 9:04 PM PDT</span>&nbsp;Between 7:14 PM and 8:39 PM PDT we experienced increased error rates and latencies for API requests in the AP-NORTHEAST-1 Region. The issue has been resolved and the service is operating normally.</div>","service":"redshift-ap-northeast-1"},{"service_name":"AWS Storage Gateway (Tokyo)","summary":"[RESOLVED] Elevated Error Rate","date":"1476418222","status":1,"details":"","description":"<div><span class=\"yellowfg\"> 8:20 PM PDT</span>&nbsp;We are investigating elevated latencies and error rates in the AP-NORTHEAST-1 Region.</div><div><span class=\"yellowfg\"> 9:10 PM PDT</span>&nbsp;Between 7:10 PM and 8:52 PM PDT, we experienced elevated latencies and error rates in the AP-NORTHEAST-1 Region. The issue has been resolved and the service is operating normally.</div>","service":"storagegateway-ap-northeast-1"},{"service_name":"AWS Management Console","summary":"[RESOLVED] Elevated error rates","date":"1476873096","status":1,"details":"","description":"<div><span class=\"yellowfg\"> 2:43 AM PDT</span>&nbsp;We are investigating increased error rates for the AWS Management Console.</div><div><span class=\"yellowfg\"> 3:31 AM PDT</span>&nbsp;Between 2:15 AM and 3:22 AM PDT we experienced increased API error rates and latencies accessing the AWS Management Console in the US-EAST-1 Region. This issue has been resolved and the service is operating normally.</div>","service":"management-console"},{"service_name":"Amazon Elastic Compute Cloud (N. Virginia)","summary":"[RESOLVED] Increased API Error Rates","date":"1476873171","status":1,"details":"","description":"<div><span class=\"yellowfg\"> 2:49 AM PDT</span>&nbsp;We are investigating increased API error rates in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 3:32 AM PDT</span>&nbsp;Between 2:15 AM and 3:22 AM PDT we experienced increased API error rates in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>","service":"ec2-us-east-1"},{"service_name":"Amazon CloudFront","summary":"[RESOLVED] Elevated Cloudfront API Error Rates","date":"1476873351","status":1,"details":"","description":"<div><span class=\"yellowfg\"> 3:22 AM PDT</span>&nbsp;We are investigating elevated error rates. End-user requests for content from our edge locations are not affected by this issue and are being served normally.</div><div><span class=\"yellowfg\"> 3:35 AM PDT</span>&nbsp;Between 2:15 AM and 3:22 AM PDT we experienced elevated error rates. End-user requests for content from our edge locations are not affected by this issue and are being served normally. This issue has been resolved and the service is operating normally.</div>","service":"cloudfront"},{"service_name":"Amazon Elastic Compute Cloud (N. Virginia)","summary":"[RESOLVED] Increased API Error Rates","date":"1476974656","status":1,"details":"","description":"<div><span class=\"yellowfg\"> 7:16 AM PDT</span>&nbsp;We are investigating increased API error rates in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 7:44 AM PDT</span>&nbsp;Between 6:53 AM and 7:33 AM PDT we experienced increased API error rates and latencies in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>","service":"ec2-us-east-1"},{"service_name":"Amazon Elastic Compute Cloud (N. Virginia)","summary":"[RESOLVED] API Endpoint","date":"1477057011","status":1,"details":"","description":"<div><span class=\"yellowfg\"> 5:15 AM PDT</span>&nbsp;We are investigating elevated errors resolving the DNS hostname used to access the EC2 APIs.</div><div><span class=\"yellowfg\"> 5:51 AM PDT</span>&nbsp;We have identified the root cause of the issue causing errors resolving the DNS hostname used to access the EC2 APIs and are currently working to resolve.</div><div><span class=\"yellowfg\"> 6:13 AM PDT</span>&nbsp;We are continuing to work to resolve the root cause of the issue. Some EC2 instances are additionally observing failures resolving particular DNS hostnames.</div><div><span class=\"yellowfg\"> 6:36 AM PDT</span>&nbsp;Between 4:31 AM and 6:10 AM PDT we experienced errors resolving the DNS hostnames used to access EC2 APIs in the US-EAST-1 Region. Additionally, some EC2 instances observed failures resolving particular external DNS hostnames. This issue has been resolved and the service is operating normally.</div>","service":"ec2-us-east-1"},{"service_name":"Amazon Simple Email Service (N. Virginia)","summary":"[RESOLVED] Elevated email-sending API error rates","date":"1477057489","status":1,"details":"","description":"<div><span class=\"yellowfg\"> 5:18 AM PDT</span>&nbsp;We are currently investigating elevated error rates for our email-sending APIs in the US-EAST-1 Region. This includes the calls made to the SMTP endpoint.</div><div><span class=\"yellowfg\"> 6:06 AM PDT</span>&nbsp;We have identified the root cause of the issue causing elevated error rates for our email-sending SMTP endpoint in the US-EAST-1 Region and we are currently working to resolve.</div><div><span class=\"yellowfg\"> 6:42 AM PDT</span>&nbsp;Between 4:31 AM and 6:10 AM PDT we experienced elevated error rates for our email-sending SMTP endpoint in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>","service":"ses-us-east-1"},{"service_name":"Amazon Elastic Load Balancing (Ohio)","summary":"[RESOLVED] Increased API Error Rates","date":"1477958887","status":1,"details":"","description":"<div><span class=\"yellowfg\"> 4:44 PM PDT</span>&nbsp;We are investigating increased error rates for creation of new load balancers and registration of back-end instances in the US-EAST-2 Region. Connectivity to existing load balancers is not affected.</div><div><span class=\"yellowfg\"> 5:08 PM PDT</span>&nbsp;Between 4:00 PM and 4:57 PM PDT, we experienced increased API error rates in the US-EAST-2 Region. The issue has been resolved and the service is operating normally. Connectivity to existing load balancers was not affected.</div>","service":"elb-us-east-2"},{"service_name":"Amazon WorkSpaces (Singapore)","summary":"[RESOLVED] Increased connection error rates","date":"1477972315","status":1,"details":"","description":"<div><span class=\"yellowfg\"> 8:27 PM PDT</span>&nbsp;We are investigating increased error rates for connections to Amazon WorkSpaces using Multi-Factor Authentication in the AP-SOUTHEAST-1 Region. Existing connections are not affected.</div><div><span class=\"yellowfg\"> 8:51 PM PDT</span>&nbsp;Between 1:21 PM PDT and 8:35 PM PDT we experienced increased error rates for connections to Amazon WorkSpaces using Multi-Factor Authentication in the AP-SOUTHEAST-1 Region. Existing connections were not affected. The issue has been resolved and the service is operating normally. </div>","service":"workspaces-ap-southeast-1"},{"service_name":"Amazon WorkSpaces (Sydney)","summary":"[RESOLVED] Increased connection error rates","date":"1477972406","status":1,"details":"","description":"<div><span class=\"yellowfg\"> 8:29 PM PDT</span>&nbsp;We are investigating increased error rates for connections to Amazon WorkSpaces using Multi-Factor Authentication in the AP-SOUTHEAST-2 Region. Existing connections are not affected.</div><div><span class=\"yellowfg\"> 8:53 PM PDT</span>&nbsp;Between 4:52 PM PDT and 8:26 PM PDT we experienced increased error rates for connections to Amazon WorkSpaces using Multi-Factor Authentication in the AP-SOUTHEAST-2 Region. Existing connections were not affected. The issue has been resolved and the service is operating normally.</div>","service":"workspaces-ap-southeast-2"},{"service_name":"Amazon CloudWatch Events (N. Virginia)","summary":"[RESOLVED] Scheduled-event Delivery Delays ","date":"1478219425","status":1,"details":"","description":"<div><span class=\"yellowfg\"> 4:10 PM PDT</span>&nbsp;We are investigating scheduled-event delivery delays in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 5:08 PM PDT</span>&nbsp;We have identified the cause of scheduled-event delivery delays in the US-EAST-1 Region and are working towards resolution. No events have been lost.</div><div><span class=\"yellowfg\"> 5:30 PM PDT</span>&nbsp;Between 1:15 PM and 5:20 PM PDT we experienced scheduled-event delivery delays in the US-EAST-1 Region. The issue has been resolved and the service is operating normally. No events were lost.</div>","service":"cloudwatchevents-us-east-1"},{"service_name":"AWS Lambda (N. Virginia)","summary":"[RESOLVED] Increased Invoke Latencies","date":"1478273474","status":1,"details":"","description":"<div><span class=\"yellowfg\"> 8:31 AM PDT</span>&nbsp;Between November 3 1:15 PM and November 4 8:13 AM PDT we experienced delayed invoke times for scheduled-event delivery in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>","service":"lambda-us-east-1"},{"service_name":"Amazon Elastic Load Balancing (N. Virginia)","summary":"[RESOLVED] Increased API Error Rates ","date":"1478822751","status":1,"details":"","description":"<div><span class=\"yellowfg\"> 3:35 PM PST</span>&nbsp;We are investigating increased error rates and latencies for the ELB APIs in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 4:05 PM PST</span>&nbsp;Between 3:23 PM and 3:51 PM PST we experienced increased error rates and latencies for the ELB APIs in the US-EAST-1 Region. Connectivity to load balancers was not affected. The issue has been resolved and the service is operating normally.</div>","service":"elb-us-east-1"},{"service_name":"AWS IoT (N. Virginia)","summary":"[RESOLVED] Increased Error Rates","date":"1479253181","status":1,"details":"","description":"<div><span class=\"yellowfg\">12:28 PM PST</span>&nbsp;We are experiencing increased API error rates to the GetThingShadow API in the US-EAST-1 Region. We have identified the root cause and are working towards recovery.</div><div><span class=\"yellowfg\"> 1:24 PM PST</span>&nbsp;We continue to investigate increased API error rates to the GetThingShadow API in the US-EAST-1 Region. The root cause has been identified and we continue to work towards resolution.</div><div><span class=\"yellowfg\"> 2:31 PM PST</span>&nbsp;We are beginning to see recovery of GetThingShadow API operations in the US-EAST-1 Region. We continue to work towards complete resolution.</div><div><span class=\"yellowfg\"> 3:39 PM PST</span>&nbsp;Between 9:00 AM and 3:30 PM PST we experienced increased error rates for the GetThingShadow API in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>","service":"awsiot-us-east-1"},{"service_name":"Amazon Elastic Compute Cloud (Oregon)","summary":"[RESOLVED] Increased API Error Rates","date":"1479431371","status":1,"details":"","description":"<div><span class=\"yellowfg\"> 4:28 PM PST</span>&nbsp;We are investigating increased error rates and latencies for the EC2 APIs in the US-WEST-2 Region.</div><div><span class=\"yellowfg\"> 5:09 PM PST</span>&nbsp;Between 3:51 PM and 4:57 PM PST we experienced increased error rates and latencies for the EC2 APIs in the US-WEST-2 Region. The issue has been resolved and the service is operating normally.</div>","service":"ec2-us-west-2"},{"service_name":"Amazon Elastic Compute Cloud (Oregon)","summary":"[RESOLVED] Increased Launch Error Rates","date":"1479766173","status":1,"details":"","description":"<div><span class=\"yellowfg\"> 1:45 PM PST</span>&nbsp;We are investigating increased error rates for new instance launches in the US-WEST-2 Region.</div><div><span class=\"yellowfg\"> 2:09 PM PST</span>&nbsp;Between 1:16 PM and 2:04 PM PST we experienced increased error rates for some APIs and new instance launches in the US-WEST-2 Region. The issue has been resolved and the service is operating normally.</div>","service":"ec2-us-west-2"},{"service_name":"Amazon Elastic Compute Cloud (N. Virginia)","summary":"[RESOLVED] Increased API Error Rates","date":"1481025161","status":1,"details":"","description":"<div><span class=\"yellowfg\"> 3:52 AM PST</span>&nbsp;Between 3:01 AM and 3:32 AM PST we experienced increased API error rates in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>","service":"ec2-us-east-1"},{"service_name":"AWS Identity and Access Management (N. Virginia)","summary":"[RESOLVED] Delays for User and Policy Updates","date":"1481033166","status":1,"details":"","description":"<div><span class=\"yellowfg\"> 6:06 AM PST</span>&nbsp;Between 4:25 AM to 5:25 AM PST we experienced increased propagation delays in delivering changes made to IAM. Previously created credentials were not impacted during this period. The issue has been resolved and the service is operating normally.</div>","service":"iam-us-east-1"},{"service_name":"Amazon Elastic Compute Cloud (Sao Paulo)","summary":"[RESOLVED] Increased Launch Error Rates","date":"1481194055","status":1,"details":"","description":"<div><span class=\"yellowfg\"> 2:47 AM PST</span>&nbsp;We are investigating increased error rates for new launches in a single Availability Zone in the SA-EAST-1 Region.</div><div><span class=\"yellowfg\"> 3:24 AM PST</span>&nbsp;Between 1:45 AM and 3:16 AM PST we experienced increased error rates for EBS-backed instance launches in a single Availability Zone in the SA-EAST-1 Region. Existing instances were unaffected. The issue has been resolved and the service is operating normally.</div>","service":"ec2-sa-east-1"},{"service_name":"Amazon CloudFront","summary":"[RESOLVED] S3 Acceleration Upload Errors","date":"1481246209","status":1,"details":"","description":"<div><span class=\"yellowfg\"> 5:17 PM PST</span>&nbsp;We are investigating errors uploading content to Amazon S3 via S3 Transfer Acceleration. Uploads using the Amazon S3 regional endpoints are operating normally. </div><div><span class=\"yellowfg\"> 6:06 PM PST</span>&nbsp;We have identified the cause of the increased error rates for uploading content to Amazon S3 via S3 Transfer Acceleration and are working towards resolution. We are starting to see recovery for uploading content to Amazon S3 in the US-EAST-1 Region via S3 Transfer Acceleration. Uploads using the Amazon S3 regional endpoints continue to operate normally. </div><div><span class=\"yellowfg\"> 6:41 PM PST</span>&nbsp;We have identified the cause of the increased error rates for uploading content to Amazon S3 via S3 Transfer Acceleration and working towards resolution. We are starting to see recovery for uploading content to Amazon S3 in multiple AWS Regions via S3 Transfer Acceleration. Uploads using the Amazon S3 regional endpoints continue to operate normally.</div><div><span class=\"yellowfg\"> 7:54 PM PST</span>&nbsp;Between 3:58 PM and 7:45 PM PST, customers would have experienced errors uploading content to Amazon S3 via S3 Transfer Acceleration. Content delivery through Amazon CloudFront and uploads using the Amazon S3 regional endpoints were operating normally during this time. The issue has been resolved and the service is operating normally.</div>","service":"cloudfront"},{"service_name":"AWS Internet Connectivity (Mumbai)","summary":"[RESOLVED] Network Connectivity","date":"1481570332","status":1,"details":"","description":"<div><span class=\"yellowfg\">11:19 AM PST</span>&nbsp;We are investigating an issue with an external provider outside of our network. This may be impacting Internet connectivity between some customer networks and the AP-SOUTH-1 Region. Connectivity to instances and services within the Region is not impacted by the event.</div><div><span class=\"yellowfg\">12:06 PM PST</span>&nbsp;Between 10:22 AM and 11:43 AM PST we experienced an Internet connectivity issue with an external provider outside of our network which affected traffic from some end-user networks and the AP-SOUTH-1 Region. Connectivity to instances and services within the Region was not impacted by the event. The issue has been resolved and connectivity has been restored.</div>","service":"internetconnectivity-ap-south-1"},{"service_name":"Amazon WorkMail (Oregon)","summary":"[RESOLVED] Delayed WorkMail email delivery","date":"1481674681","status":1,"details":"","description":"<div><span class=\"yellowfg\"> 4:18 PM PST</span>&nbsp;We are investigating increased WorkMail SMTP E-Mail processing time in the US-WEST-2 Region resulting in a delay of received email to client applications.</div><div><span class=\"yellowfg\"> 4:34 PM PST</span>&nbsp;Between 2:40 PM and 4:10 PM PDT we experienced increased WorkMail SMTP E-Mail processing time in the US-WEST-2 Region. The issue has been resolved and the service is operating normally.</div>","service":"workmail-us-west-2"},{"service_name":"Amazon Elastic Compute Cloud (Ireland)","summary":"[RESOLVED] DNS Resolution Issues","date":"1481766608","status":1,"details":"","description":"<div><span class=\"yellowfg\"> 5:50 PM PST</span>&nbsp;\r\n\r\nWe are investigating DNS resolution issues from some instances in the EU-WEST-1 Region.</div><div><span class=\"yellowfg\"> 6:29 PM PST</span>&nbsp;We have identified the root cause of the DNS resolution issues in the EU-WEST-1 Region and continue working towards resolution.</div><div><span class=\"yellowfg\"> 7:00 PM PST</span>&nbsp;We continue working through DNS resolution issues in the EU-WEST-1 Region. DNS resolution for public endpoints from the Internet are operating normally.</div><div><span class=\"yellowfg\"> 7:07 PM PST</span>&nbsp;We continue working through DNS resolution issues which is now affecting a single Availability Zone in the EU-WEST-1 Region. DNS resolution for public endpoints from the Internet continue operating normally. </div><div><span class=\"yellowfg\"> 7:17 PM PST</span>&nbsp;Between 5:14 PM and 7:09 PM PST we experienced elevated DNS resolution issues for some instances in the EU-WEST-1 Region. DNS resolution for public endpoints from the Internet was not impacted during this time. The issue has been resolved and the service is operating normally.</div>","service":"ec2-eu-west-1"},{"service_name":"Amazon Elastic Compute Cloud (Singapore)","summary":"[RESOLVED] Increased Launch Error Rates","date":"1483481057","status":1,"details":"","description":"<div><span class=\"yellowfg\"> 2:04 PM PST</span>&nbsp;We are investigating increased error rates for new instance launches in the AP-SOUTHEAST-1 Region.</div><div><span class=\"yellowfg\"> 2:28 PM PST</span>&nbsp;Between 1:40 PM and 2:09 PM PST we experienced increased error rates for new instance launches the AP-SOUTHEAST-1 Region. Connectivity between EC2 and VPC Endpoints for Amazon S3 was also impacted. The issue has been resolved and the service is operating normally.</div>","service":"ec2-ap-southeast-1"},{"service_name":"Amazon Simple Storage Service (Singapore)","summary":"[RESOLVED] Increased Error Rates","date":"1483483774","status":1,"details":"","description":"<div><span class=\"yellowfg\"> 2:49 PM PST</span>&nbsp;Between 1:40 PM and 2:09 PM PST we experienced increased error rates in the AP-SOUTHEAST-1 Region. The issue has been resolved and the service is operating normally.</div>","service":"s3-ap-southeast-1"},{"service_name":"Amazon Simple Storage Service (N. California)","summary":"[RESOLVED] Increased Error Rates","date":"1483733762","status":1,"details":"","description":"<div><span class=\"yellowfg\">12:16 PM PST</span>&nbsp;We are investigating increased error rates for Amazon S3 requests in the US-WEST-1 Region.</div><div><span class=\"yellowfg\">12:41 PM PST</span>&nbsp;We have identified the cause of the elevated error rates and are working towards recovery.</div><div><span class=\"yellowfg\">12:53 PM PST</span>&nbsp;Between 10:12 AM PST and 12:40 PM PST some customers experienced brief spikes of errors within the US-WEST-1 Region. We have resolved the issue and the service is operating normally.</div>","service":"s3-us-west-1"},{"service_name":"AWS Internet Connectivity (Tokyo)","summary":"[RESOLVED] Network Connectivity","date":"1483987609","status":1,"details":"","description":"<div><span class=\"yellowfg\">10:47 AM PST</span>&nbsp;Between 9:15 AM and 9:36 AM PST we experienced an Internet connectivity issue with an external provider outside of our network. This issue impacted some connectivity between services in the AP-NORTHEAST-1 Region and other Regions. The issue has been resolved and connectivity has been restored.</div>","service":"internetconnectivity-ap-northeast-1"},{"service_name":"Amazon Elastic Compute Cloud (N. Virginia)","summary":"[RESOLVED] Increased API Error Rates","date":"1484082995","status":1,"details":"","description":"<div><span class=\"yellowfg\"> 1:16 PM PST</span>&nbsp;We are investigating increased error rates for the EC2 Management Console and some Describe APIs in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 1:59 PM PST</span>&nbsp;Between 12:36 PM and 1:50 PM PST, we experienced elevated error rates for the EC2 Management Console and some Describe APIs in the US-EAST-1 Region. This issue has been resolved and the service is operating normally.</div>","service":"ec2-us-east-1"},{"service_name":"AWS CodeDeploy (N. Virginia)","summary":"[RESOLVED] Increased Deployment Error Rates","date":"1484083636","status":1,"details":"","description":"<div><span class=\"yellowfg\"> 1:27 PM PST</span>&nbsp;We are investigating increased deployment error rates in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 1:56 PM PST</span>&nbsp;Between 12:40 and 1:50 PM PST we experienced increased deployment error rates in the US-EAST-1 Region. The issue has been resolved and the service is operating normally. </div>","service":"codedeploy-us-east-1"},{"service_name":"Amazon Elastic Compute Cloud (Oregon)","summary":"[RESOLVED] Increased API Error Rates","date":"1484091123","status":1,"details":"","description":"<div><span class=\"yellowfg\"> 3:34 PM PST</span>&nbsp;Between 2:35 PM and 3:16 PM PST we experienced elevated error rates for the EC2 Management Console and some Describe APIs in the US-WEST-2 Region. This issue has been resolved and the service is operating normally. </div>","service":"ec2-us-west-2"},{"service_name":"Amazon Elastic Compute Cloud (N. Virginia)","summary":"[RESOLVED] Increased Console Error Rates","date":"1484933886","status":1,"details":"","description":"<div><span class=\"yellowfg\"> 9:38 AM PST</span>&nbsp;We are investigating increased errors when creating or modifying security groups via the EC2 Management Console in the US-EAST-1 Region. Creating or modifying security groups via the VPC Management Console or the EC2 CLI is not affected by this issue.</div><div><span class=\"yellowfg\"> 9:48 AM PST</span>&nbsp;Between 8:05 AM and 9:40 AM PST we experienced increased errors when creating or modifying security groups via the EC2 Management Console in the US-EAST-1 Region. Creating or modifying security groups via the VPC Management Console or the EC2 CLI was not affected by this issue. The issue has been resolved and the service is operating normally.</div>","service":"ec2-us-east-1"},{"service_name":"Amazon Elastic MapReduce (N. Virginia)","summary":"[RESOLVED] Delays in starting and terminating clusters ","date":"1485255561","status":1,"details":"","description":"<div><span class=\"yellowfg\"> 2:59 AM PST</span>&nbsp;We are investigating delays in starting and terminating clusters in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 3:31 AM PST</span>&nbsp;We are continuing to investigate delays in starting and terminating clusters in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 4:23 AM PST</span>&nbsp;Between 12:40 AM and 3:42 AM PST, we experienced delays in running steps, as well as starting and terminating Amazon EMR clusters in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>","service":"emr-us-east-1"},{"service_name":"Amazon Elastic Compute Cloud (Ireland)","summary":"[RESOLVED] Increased API Error Rates","date":"1485987835","status":1,"details":"","description":"<div><span class=\"yellowfg\"> 2:24 PM PST</span>&nbsp;We are investigating elevated API error rates in the EU-WEST-1 Region. Running instances are not impacted.</div><div><span class=\"yellowfg\"> 2:46 PM PST</span>&nbsp;Between 2:05 PM and 2:42 PM PST, we experienced elevated API error rates for operations against a single Availability Zone in the EU-WEST-1 Region. Running instances were not impacted. This issue has been resolved and the service is operating normally.</div>","service":"ec2-eu-west-1"},{"service_name":"AWS Management Console","summary":"[RESOLVED] Elevated Error Rates","date":"1486349208","status":1,"details":"","description":"<div><span class=\"yellowfg\"> 6:47 PM PST</span>&nbsp;We are investigating elevated error rates for the AWS Management Console in the SA-EAST-1 Region.</div><div><span class=\"yellowfg\"> 7:21 PM PST</span>&nbsp;Between 5:44 PM and 6:46 PM PST, some users experienced elevated error rates while accessing the AWS Management Console in the SA-EAST-1 Region. The issue has been resolved and the Console is operating normally.</div>","service":"management-console"},{"service_name":"Amazon CloudFront","summary":"[RESOLVED] Regional Edge Cache Connectivity","date":"1486421405","status":1,"details":"","description":"<div><span class=\"yellowfg\"> 2:50 PM PST</span>&nbsp;We are currently investigating intermittent errors connecting to some custom non-S3 Origins from our Regional Edge Caches.</div><div><span class=\"yellowfg\"> 3:55 PM PST</span>&nbsp;We are investigating an intermittent issue resulting in errors resolving origin domains hosted at an external provider outside of the AWS network. </div><div><span class=\"yellowfg\"> 4:40 PM PST</span>&nbsp;Between 1:00 PM and 3:35 PM PST some Non-S3 CloudFront origins domains hosted by a single external DNS provider experienced intermittent resolution errors. The errors have now recovered, and all CloudFront origin DNS resolution is working normally.</div>","service":"cloudfront"},{"service_name":"Amazon Elastic Compute Cloud (Ireland)","summary":"[RESOLVED] Increased API Error Rates","date":"1486575974","status":1,"details":"","description":"<div><span class=\"yellowfg\"> 9:46 AM PST</span>&nbsp;Between 9:16 AM and 9:33 AM PST we experienced increased error rates and latencies for the EC2 APIs in a single Availability Zone in the EU-WEST-1 Region. The issue has been resolved and the service is operating.</div>","service":"ec2-eu-west-1"},{"service_name":"Amazon Elastic Compute Cloud (N. California)","summary":"[RESOLVED] Network Connectivity","date":"1486690069","status":1,"details":"","description":"<div><span class=\"yellowfg\"> 5:28 PM PST</span>&nbsp;We are investigating network connectivity issues in a single Availability Zone in the US-WEST-1 Region.</div><div><span class=\"yellowfg\"> 6:15 PM PST</span>&nbsp;We continue to investigate network connectivity issues for instances and failures of newly launched instances in a single Availability Zone in the US-WEST-1 Region.</div><div><span class=\"yellowfg\"> 7:09 PM PST</span>&nbsp;We have identified the root cause of the network connectivity issues for instances and failures of newly launched instances in a single Availability Zone in the US-WEST-1 Region. Connectivity to some instances has been restored and we continue to work on the remaining instances.</div><div><span class=\"yellowfg\"> 8:10 PM PST</span>&nbsp;Between 4:52 PM and 7:49 PM PST we experienced network connectivity issues for instances and failures of newly launched instances in a single Availability Zone in the US-WEST-1 Region. The issue has been resolved and the service is operating normally.</div>","service":"ec2-us-west-1"},{"service_name":"AWS Lambda (N. California)","summary":"[RESOLVED] Increased error rates and elevated latencies","date":"1486697386","status":1,"details":"","description":"<div><span class=\"yellowfg\"> 7:29 PM PST</span>&nbsp;We can confirm increased error rates and elevated latencies for AWS Lambda requests in the US-WEST-1 Region. Newly created functions and console editing are also affected. We have identified the root cause and working to resolve the issue. </div><div><span class=\"yellowfg\"> 8:12 PM PST</span>&nbsp;Between 4:52 PM and 8:01 PM PST we experienced elevated error rates and latencies in the US-WEST-1 Region. The issue has been resolved and the service is operating normally. </div>","service":"lambda-us-west-1"},{"service_name":"Amazon CloudWatch (Sydney)","summary":"[RESOLVED] Delayed metrics extracted from logs in AP-SOUTHEAST-2","date":"1487382012","status":1,"details":"","description":"<div><span class=\"yellowfg\"> 5:40 PM PST</span>&nbsp;We are investigating increased delays for CloudWatch metric data extracted from CloudWatch Logs in the AP-SOUTHEAST-2 Region. CloudWatch alarms dependent on metrics extracted from logs may transition into \"INSUFFICIENT_DATA\" state. </div><div><span class=\"yellowfg\"> 5:51 PM PST</span>&nbsp;Between 4:38 PM and 5:43 PM PST, some customers experienced increased delays for CloudWatch metrics extracted from CloudWatch Logs in the AP-SOUTHEAST-2 Region. CloudWatch alarms on delayed metrics may have transitioned into INSUFFICIENT_DATA state. We have resolved the issue. Delayed metrics are backfilled in CloudWatch console graphs and for API retrieval. The service is operating normally.</div>","service":"cloudwatch-ap-southeast-2"},{"service_name":"AWS Management Console","summary":"[RESOLVED] Increased Console Error Rates","date":"1487652711","status":1,"details":"","description":"<div><span class=\"yellowfg\"> 8:52 PM PST</span>&nbsp;We are currently investigating increased error rates for the AWS Management Console in the US-WEST-2 Region.</div><div><span class=\"yellowfg\"> 9:12 PM PST</span>&nbsp;Between 7:49 PM and 8:59 PM PST we experienced increased error rates for the AWS Management Console in the US-WEST-2 Region. The issue has been resolved and the service is operating normally.</div>","service":"management-console"},{"service_name":"Amazon Elastic Compute Cloud (Oregon)","summary":"[RESOLVED] Increased API Error Rates","date":"1487678496","status":1,"details":"","description":"<div><span class=\"yellowfg\"> 4:01 AM PST</span>&nbsp;We are investigating increased API error rates and latencies in a single Availability Zone in the US-WEST-2 Region.</div><div><span class=\"yellowfg\"> 4:14 AM PST</span>&nbsp;Between 3:29 AM and 4:04 AM PST we experienced increased API error rates and latencies in a single Availability Zone in the US-WEST-2 Region. The issue has been resolved and the service is operating normally.</div>","service":"ec2-us-west-2"},{"service_name":"Amazon Elastic Compute Cloud (N. Virginia)","summary":"[RESOLVED] Elevated Error Rates","date":"1487869990","status":1,"details":"","description":"<div><span class=\"yellowfg\"> 9:13 AM PST</span>&nbsp;We are investigating degraded performance for a small number of EBS volumes in a Single Availability Zone in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 9:46 AM PST</span>&nbsp;We are investigating degraded performance for a small number of EBS volumes in a single Availability Zone. Existing instances using affected EBS volumes may experience degraded I/O performance. We are also experiencing elevated error rates impacting attempts to launch or start EBS backed instances using RunInstances or StartInstances.</div><div><span class=\"yellowfg\">10:45 AM PST</span>&nbsp;We are continuing to investigate degraded performance for a small number of EBS volumes in a single Availability Zone. Existing instances using affected EBS volumes may experience degraded I/O performance. We are also experiencing elevated error rates impacting attempts to launch or start EBS backed instances using RunInstances or StartInstances. We have narrowed the scope of impact and are working to identify the cause of this issue.</div><div><span class=\"yellowfg\">11:54 AM PST</span>&nbsp;We have posted an update to the Personal Health Dashboard for impacted customers. Please visit the dashboard to determine whether or not you are impacted.</div><div><span class=\"yellowfg\">12:25 PM PST</span>&nbsp;Beginning at 8:43 AM PST we experienced degraded performance for a small number of EBS volumes in a single Availability Zone. Existing instances using affected EBS volumes experienced degraded I/O performance. We also experienced elevated error rates impacting attempts to launch or start EBS backed instances using RunInstances or StartInstances in the affected Availability Zone. Access to the majority of EBS volumes impacted by this issue has been restored. We have posted an update to the Personal Health Dashboard for customers with volumes that were impacted, and we are continuing to work to remediate all volumes. </div>","service":"ec2-us-east-1"},{"service_name":"Amazon Simple Storage Service (N. Virginia)","summary":"[RESOLVED] Increased Error Rates","date":"1488310593","status":"3","details":"","description":"<div><span class=\"yellowfg\">11:37 AM PST</span>&nbsp;We can confirm high error rates for requests made to S3 in the US-EAST-1 Region. We've identified the issue and are working to restore normal operations. </div><div><span class=\"yellowfg\">12:54 PM PST</span>&nbsp;We are seeing recovery for S3 object retrievals, listing and deletions. We continue to work on recovery for adding new objects to S3 and expect to start seeing improved error rates within the hour.</div><div><span class=\"yellowfg\"> 1:13 PM PST</span>&nbsp;S3 object retrieval, listing and deletion are fully recovered now. We are still working to recover normal operations for adding new objects to S3.</div><div><span class=\"yellowfg\"> 2:11 PM PST</span>&nbsp;As of 1:49 PM PST, we are fully recovered for operations for adding new objects in S3, which was our last operation showing a high error rate. The Amazon S3 service is operating normally.</div><br>Additional details regarding the Amazon S3 Service Disruption on Feb 28th are now available <a href=\"https://aws.amazon.com/message/41926/\">here.</a>","service":"s3-us-standard"},{"service_name":"Amazon Elastic Load Balancing (N. Virginia)","summary":"[RESOLVED] Increased Error Rates","date":"1488307639","status":1,"details":"","description":"<div><span class=\"yellowfg\">11:38 AM PST</span>&nbsp;We can confirm increased error rates for the ELB APIs and delayed provisioning of new load balancers in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 4:06 PM PST</span>&nbsp;Between 9:37 AM and 4:00 PM PST we experienced increased error rates for the ELB APIs and delayed provisioning of new load balancers in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>","service":"elb-us-east-1"},{"service_name":"AWS Lambda (N. Virginia)","summary":"[RESOLVED] Increased Error Rates","date":"1488310576","status":2,"details":"","description":"<div><span class=\"yellowfg\">11:38 AM PST</span>&nbsp;We are confirming increased error rates and elevated latencies for AWS Lambda requests in the US-EAST-1 Region. Newly created functions and console editing are also affected.</div><div><span class=\"yellowfg\"> 2:47 PM PST</span>&nbsp;We continue to experience elevated error rates and latencies for AWS Lambda API requests in the US-EAST-1 Region. New function creation, update of existing functions and console editing are now operating normally. We have identified the root cause and working to resolve the issue.</div><div><span class=\"yellowfg\"> 4:38 PM PST</span>&nbsp;We continue to see recovery in error rates and latencies for AWS Lambda API requests in the US-EAST-1 Region. Some customers can still experience errors in new function creation, update of existing functions and console editing. We continue to work towards full resolution.</div><div><span class=\"yellowfg\"> 7:13 PM PST</span>&nbsp;Between 9:37 AM and 6:45 PM PST we experienced elevated error rates and latencies for Lambda APIs in the US-EAST-1 Region. New function creation, update of existing functions and console editing were also impacted. The issue has been resolved and the service is operating normally. Some customers may see delays in Lambda execution of asynchronous Lambda calls as we work through the asynchronous call backlog. </div>","service":"lambda-us-east-1"},{"service_name":"Amazon Elastic Compute Cloud (N. Virginia)","summary":"[RESOLVED] Increased Error Rates","date":"1488310666","status":2,"details":"","description":"<div><span class=\"yellowfg\">11:38 AM PST</span>&nbsp;We can confirm increased error rates for the EC2 and EBS APIs and failures for launches of new EC2 instances in the US-EAST-1 Region. We are also experiencing degraded performance of some EBS Volumes in the Region.\r\n</div><div><span class=\"yellowfg\"> 1:36 PM PST</span>&nbsp;We can confirm recovery for degraded performance for some EBS Volumes in the US-EAST-1 Region. We continue to work towards recovery for the increased error rates for the EC2 and EBS APIs and failures for launches of new EC2 instances. </div><div><span class=\"yellowfg\"> 2:46 PM PST</span>&nbsp;We are starting to see recovery in error rates for the EC2 and EBS APIs and failures for launches of new EC2 instances in the US-EAST-1 Region. Some customers may see a \"Request Limit Exceeded\" error until we are fully recovered.</div><div><span class=\"yellowfg\"> 3:41 PM PST</span>&nbsp;Between 9:37 AM and 3:20 PM PST we experienced increased error rates for the EC2 and EBS APIs and failure to launch new EC2 instances in the US-EAST-1 Region. Some EBS Volumes also experienced degraded performance. Connectivity to existing EC2 instances was not affected. The issue has been resolved and the service is operating normally.</div>","service":"ec2-us-east-1"},{"service_name":"Auto Scaling (N. Virginia)","summary":"[RESOLVED] Increased Error Rates","date":"1488310699","status":3,"details":"","description":"<div><span class=\"yellowfg\">11:39 AM PST</span>&nbsp;We can confirm increased launch times for EC2 instances managed by Auto Scaling in the US-EAST-1 Region. We have identified the cause and continue to work to remediate.</div><div><span class=\"yellowfg\"> 2:42 PM PST</span>&nbsp;We are seeing improvement in the launch times for EC2 instances managed by Auto Scaling the US-EAST-1 Region. We have identified the root cause and continue to work to remediate.</div><div><span class=\"yellowfg\"> 3:34 PM PST</span>&nbsp;We continue to see improvement in the launch times for EC2 instances managed by Auto Scaling the US-EAST-1 Region. We have identified the root cause and continue to work to remediation.</div><div><span class=\"yellowfg\"> 6:10 PM PST</span>&nbsp;Between 9:37 AM and 6:04 PM we experienced elevated launch and terminate times for EC2 instances managed by Auto Scaling in the US-EAST-1 Region. Existing running EC2 instances were not impacted. The issue has been resolved and the service is operating normally. </div>","service":"autoscaling-us-east-1"},{"service_name":"Amazon Relational Database Service (N. Virginia)","summary":"[RESOLVED] Increased Error Rates","date":"1488310704","status":1,"details":"","description":"<div><span class=\"yellowfg\">11:39 AM PST</span>&nbsp;We can confirm increased failure rates in instance creates and modifications, and delays in database backups. A small number of customers are unable to access their instances.</div><div><span class=\"yellowfg\"> 1:47 PM PST</span>&nbsp;We are starting to see recovery for instance create and modification operations as well as backup rates in the US-EAST-1 Region. We have identified the root cause and are working to remediate the issue.</div><div><span class=\"yellowfg\"> 3:05 PM PST</span>&nbsp;We continue to see recovery for instance creation and modification operations in the US-EAST-1 Region. We have identified the root cause and are continuing to make progress to resolve the issue.</div><div><span class=\"yellowfg\"> 4:37 PM PST</span>&nbsp;Between 9:37 AM and 4:30 PM PST, we experienced increased error rates for instance creation and modification operations as well as slower backup rates in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>","service":"rds-us-east-1"},{"service_name":"Amazon Elastic File System (N. Virginia)","summary":"[RESOLVED] Increased Error Rates","date":"1488310812","status":1,"details":"","description":"<div><span class=\"yellowfg\">11:43 AM PST</span>&nbsp;We can confirm increased latencies for some file operations, and we can confirm customers are unable to create new endpoints for accessing file systems in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 1:38 PM PST</span>&nbsp;Latencies for file operations in the US-EAST-1 Region are back to normal. Customers continue to be unable to mount using newly-created mount targets in US-EAST-1.</div><div><span class=\"yellowfg\"> 2:00 PM PST</span>&nbsp;Between 9:37 AM and 1:45 PM PST customers may have experienced increased latency for file operations and were unable to mount file systems using newly created mount targets in the US-EAST-1 Region. The issue has been resolved and the service is operating normally</div>","service":"elasticfilesystem-us-east-1"},{"service_name":"Amazon Simple Email Service (N. Virginia)","summary":"[RESOLVED] Increased Error Rates","date":"1488310840","status":3,"details":"","description":"<div><span class=\"yellowfg\">11:43 AM PST</span>&nbsp;We can confirm elevated error rates for our email-sending APIs in the US-EAST-1 Region. This includes the SendEmail/SendRawEmail APIs as well as calls made to the SMTP endpoint.</div><div><span class=\"yellowfg\"> 1:54 PM PST</span>&nbsp;\r\nWe are seeing recovery for our email-sending and email-receiving APIs in the US-EAST-1 Region. This includes the SendEmail/SendRawEmail APIs as well as calls made to the sending and receiving SMTP endpoints. We will continue to monitor this issue until fully recovered in US-EAST-1. </div><div><span class=\"yellowfg\"> 4:33 PM PST</span>&nbsp;Between 9:37 AM PST and 3:18 PM PST we experienced elevated error rates for our email-sending and email-receiving APIs in the US-EAST-1 Region. This included the SendEmail/SendRawEmail APIs as well as calls made to the sending and receiving SMTP endpoints. The issue has been resolved and the service is operating normally.</div>","service":"ses-us-east-1"},{"service_name":"Amazon WorkMail (N. Virginia)","summary":"[RESOLVED] Increased Error Rates","date":"1488310761","status":3,"details":"","description":"<div><span class=\"yellowfg\">11:44 AM PST</span>&nbsp;We can confirm that customers cannot open or synchronize their mailboxes with their email client or open their emails with the WorkMail web client in the US-EAST-1 Region. Incoming email deliveries are delayed.</div><div><span class=\"yellowfg\"> 2:15 PM PST</span>&nbsp;Between 9:37 AM and 1:40 PM PST we experienced elevated failure rates for mailboxes synchronization in US-EAST-1 Region. We also observed higher email delivery latencies in this region. The issue has been resolved and the service is operating normally.</div>","service":"workmail-us-east-1"},{"service_name":"AWS Elastic Beanstalk (N. Virginia)","summary":"[RESOLVED] Increased Error Rates","date":"1488311003","status":2,"details":"","description":"<div><span class=\"yellowfg\">11:44 AM PST</span>&nbsp;We can confirm elevated error rates creating, updating and terminating Elastic Beanstalk environments in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 2:14 PM PST</span>&nbsp;We are seeing recovery in creating, updating, and terminating Elastic Beanstalk environments in the US-EAST-1 Region. We will continue to monitor these actions until fully recovered. Running environments continue to operate normally.</div><div><span class=\"yellowfg\"> 2:15 PM PST</span>&nbsp;</div><div><span class=\"yellowfg\"> 4:10 PM PST</span>&nbsp;Between 9:37 AM and 3:43 PM PST, we experienced elevated error rates creating, updating, and terminating Elastic Beanstalk environments in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>","service":"elasticbeanstalk-us-east-1"},{"service_name":"AWS WAF","summary":"[RESOLVED] Increased Error Rates","date":"1488311040","status":1,"details":"","description":"<div><span class=\"yellowfg\">11:44 AM PST</span>&nbsp;We can confirm propagation of WAF rule changes are delayed to Application Load Balancers in the US-EAST-1 Region. This does not impact request inspection for previously configured WAF rules.</div><div><span class=\"yellowfg\"> 3:01 PM PST</span>&nbsp;Between 9:37 AM and 1:55 PM PST, the propagation of WAF rule changes were delayed to Application Load Balancers in US-EAST-1. WAF rules changes are propagating normally now. During this time request inspection was not impacted. The issue has been resolved and the service is operating normally.</div>","service":"awswaf"},{"service_name":"Amazon WorkDocs (N. Virginia)","summary":"[RESOLVED] Increased Error Rates","date":"1488311054","status":2,"details":"","description":"<div><span class=\"yellowfg\">11:45 AM PST</span>&nbsp;We can confirm that our web, mobile, and sync clients are experiencing increased error rates impacting file uploads, downloads, and previews in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 2:23 PM PST</span>&nbsp;Between 9:37 and 1:54 PM PST we experienced increased failure rates for file uploads, downloads, and previews in Amazon WorkDocs in the US-EAST-1 Region. We also experienced increased failure rates for new logins to WorkDocs sites. The issue has been resolved and the service is operating normally</div>","service":"workdocs-us-east-1"},{"service_name":"Amazon Redshift (N. Virginia)","summary":"[RESOLVED] Increased Error Rates","date":"1488311067","status":2,"details":"","description":"<div><span class=\"yellowfg\">11:45 AM PST</span>&nbsp;We can confirm increased error rates for create, restore, copy, unload and backup operations in the US-EAST-1 Region, and are working to remediate the issue. Running clusters are unaffected.</div><div><span class=\"yellowfg\"> 1:37 PM PST</span>&nbsp;We are starting to see recovery for copy, create and restore operations in the US-EAST-1 Region. We continue to see errors for unload and backup operations. Connectivity to existing Redshift clusters was not affected. We have identified the root cause and are working to remediate the issue.</div><div><span class=\"yellowfg\"> 2:03 PM PST</span>&nbsp;\r\n\r\nBetween 9:37 AM and 1:58 PM PST we experienced increased error rates for create, restore, copy, unload and backup operations in the US-EAST-1 Region due to S3 API errors. Connectivity to existing Redshift clusters was not affected. The issue has been resolved and the service is operating normally.</div>","service":"redshift-us-east-1"},{"service_name":"AWS CodeDeploy (N. Virginia)","summary":"[RESOLVED] Increased Error Rates","date":"1488311111","status":2,"details":"","description":"<div><span class=\"yellowfg\">11:46 AM PST</span>&nbsp;We can confirm increased deployment error rates in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 2:32 PM PST</span>&nbsp;We are seeing recovery in error rates while executing deployments in the US-EAST-1 Region. We will continue to monitor deployment error rate until fully recovered.</div><div><span class=\"yellowfg\"> 5:37 PM PST</span>&nbsp;Between 9:37 AM PST and 5:20 PM PST we experienced an increased error rate while executing deployments, in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>","service":"codedeploy-us-east-1"},{"service_name":"AWS CodeCommit (N. Virginia)","summary":"[RESOLVED] Increased Error Rates","date":"1488311111","status":2,"details":"","description":"<div><span class=\"yellowfg\">11:46 AM PST</span>&nbsp;We can confirm increased error rates in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 2:17 PM PST</span>&nbsp;Between 9:37 AM and 1:53 PM PST we experienced increased error rates for AWS CodeCommit API and Git operations in the US-EAST-1 region. The issue has been resolved and the service is operating normally.</div>","service":"codecommit-us-east-1"},{"service_name":"Amazon CloudWatch (N. Virginia)","summary":"[RESOLVED] Increased Error Rates","date":"1488311170","status":1,"details":"","description":"<div><span class=\"yellowfg\">11:46 AM PST</span>&nbsp;We can confirm impacts to the GetMetricStatistics API for metrics data more than two days old in the US-EAST-1 Region. Additionally, CloudWatch dashboards are currently unavailable. We are actively working to resolve the issue. Metric Graphs and GetMetricStatistic API requests configured to retrieve less than two days of data are working normally.</div><div><span class=\"yellowfg\"> 1:48 PM PST</span>&nbsp;CloudWatch is starting to see service recovery. CloudWatch Metrics APIs fault rates and availability of data more than two days old has now recovered. CloudWatch Dashboards are now available for read only access. CloudWatch logs customers continue to experience elevated API faults and delayed log events. We are actively working to resolve the issue. </div><div><span class=\"yellowfg\"> 2:29 PM PST</span>&nbsp;Between 9:37 AM and 1:17 PM PST, customers experienced elevated faults when making GetMetricStatistics API requests for metrics data more than two days old in the US-EAST-1 Region and CloudWatch Dashboards were unavailable. Between 9:37 AM and 2:05 PM PST, CloudWatch logs customers experienced elevated API faults and delayed log events. The issue has been resolved and the service is operating normally.</div>","service":"cloudwatch-us-east-1"},{"service_name":"AWS CodeBuild (N. Virginia)","summary":"[RESOLVED] Increased Error Rates","date":"1488311195","status":2,"details":"","description":"<div><span class=\"yellowfg\">11:47 AM PST</span>&nbsp;We can confirm increased error rates in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 3:41 PM PST</span>&nbsp;We are seeing recovery in error rates while executing builds in the US-EAST-1 Region. We will continue to monitor build error rates until fully recovered. </div><div><span class=\"yellowfg\"> 4:01 PM PST</span>&nbsp;Between 9:37 AM and 3:48 PST, we experienced elevated error rates for APIs and builds in the US-EAST-1 Region. The errors have recovered and the service is now operating normally.</div>","service":"codebuild-us-east-1"},{"service_name":"AWS OpsWorks Stacks (N. Virginia)","summary":"[RESOLVED] Increased Error Rates","date":"1488311185","status":2,"details":"","description":"<div><span class=\"yellowfg\">11:47 AM PST</span>&nbsp;We can confirm instance launches and deployments are impaired in the US-EAST-1 Region. Server launches for Chef Automate and operations like backups are also impaired.</div><div><span class=\"yellowfg\"> 2:33 PM PST</span>&nbsp;We are seeing recovery in launching instances in the US-EAST-1 Region. We will continue to monitor these actions until fully recovered. Running instances continue to operate normally.</div><div><span class=\"yellowfg\"> 4:59 PM PST</span>&nbsp;A small number of customers are still experiencing instance launch and command failures in the US-EAST-1 Region. Running instances continue to operate normally.</div><div><span class=\"yellowfg\"> 6:13 PM PST</span>&nbsp;Between 9:37 AM and 6:01 PM PST we experienced elevated error rates launching instances and executing commands in the US-EAST-1 Region. Running instances were not affected. The issue has been resolved and the service is operating normally. </div>","service":"opsworks-us-east-1"},{"service_name":"AWS Key Management Service (N. Virginia)","summary":"[RESOLVED] Increased Error Rates","date":"1488311200","status":2,"details":"","description":"<div><span class=\"yellowfg\">11:47 AM PST</span>&nbsp;We can confirm increased error rates for creating keys in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 2:02 PM PST</span>&nbsp;Between 9:37 AM and 1:56 PM PST we experienced increased error rates in creating and describing keys in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>","service":"kms-us-east-1"},{"service_name":"AWS CodePipeline (N. Virginia)","summary":"[RESOLVED] Increased Error Rates","date":"1488311253","status":1,"details":"","description":"<div><span class=\"yellowfg\">11:48 AM PST</span>&nbsp;We can confirm increased error rates in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 5:04 PM PST</span>&nbsp;We are seeing recovery in error rates for pipeline executions in the US-EAST-1 Region. We will continue to monitor latency and error rates until fully recovered.</div><div><span class=\"yellowfg\"> 5:58 PM PST</span>&nbsp;Between 9:37 AM and 5:30 PM PST we experienced increased API error rates and pipeline execution latencies for AWS CodePipeline in the US-EAST-1 Region. New pipeline creation, updates of existing pipelines, and pipeline executions were impacted. The issue has been resolved and the service is operating normally.</div>","service":"codepipeline-us-east-1"},{"service_name":"Amazon Simple Workflow Service (N. Virginia)","summary":"[RESOLVED] Increased Error Rates","date":"1488311264","status":2,"details":"","description":"<div><span class=\"yellowfg\">11:48 AM PST</span>&nbsp;We can confirm increased API latency for Workflow and Activity APIs in the US-EAST-1 Region and continue to work towards resolution.</div><div><span class=\"yellowfg\"> 2:55 PM PST</span>&nbsp;We can confirm recovery for API latency for Workflow and Activity APIs in the US-EAST-1 Region. We continue to work through a backlog of workflows and continue to work towards full recovery.</div><div><span class=\"yellowfg\"> 6:20 PM PST</span>&nbsp;Between 9:37 AM and 6:15 PM PST we experienced elevated latencies for Activity and Workflow APIs in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>","service":"swf-us-east-1"},{"service_name":"AWS Storage Gateway (N. Virginia)","summary":"[RESOLVED] Increased Error Rates","date":"1488311301","status":2,"details":"","description":"<div><span class=\"yellowfg\">11:48 AM PST</span>&nbsp;We can confirm increased error rates for read/write operations in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 1:57 PM PST</span>&nbsp;We are beginning to see recovery for error rates for read/write operations in the US-EAST-1 Region. </div><div><span class=\"yellowfg\"> 2:25 PM PST</span>&nbsp;Between 9:37 AM and 2:10 PM PST we experienced increased error rates for read/write operations in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>","service":"storagegateway-us-east-1"},{"service_name":"Amazon Kinesis Analytics (N. Virginia)","summary":"[RESOLVED] Increased Error Rates","date":"1488311308","status":1,"details":"","description":"<div><span class=\"yellowfg\">11:49 AM PST</span>&nbsp;We can confirm increased delays in starting applications in the US-EAST-1 Region</div><div><span class=\"yellowfg\"> 4:36 PM PST</span>&nbsp;Between 9:37 AM and 4:10 PM PST we experienced increased error rates for the Kinesis Analytics APIs in the US-EAST-1 Region. The issue has been resolved and the service is operating normally. </div>","service":"kinesisanalytics-us-east-1"},{"service_name":"Amazon Athena (N. Virginia)","summary":"[RESOLVED] Increased Error Rates","date":"1488311328","status":3,"details":"","description":"<div><span class=\"yellowfg\">11:49 AM PST</span>&nbsp;We can confirm increased query failure rates when running queries and executing DDL statements in the in US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 1:59 PM PST</span>&nbsp;We are starting to see recovery when running SQL queries in the US-EAST-1 region. We continue to see elevated error rates when executing DDL statements in the US-EAST-1 region. </div><div><span class=\"yellowfg\"> 2:12 PM PST</span>&nbsp;We continue to see recovery when running SQL queries in the US-EAST-1 region. We are also starting to see recovery when executing DDL statements in the US-EAST-1 region.</div><div><span class=\"yellowfg\"> 3:25 PM PST</span>&nbsp;We are able to execute SQL queries normally in the US-EAST-1 region. We continue to see increasing recovery when executing DDL statements in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 3:44 PM PST</span>&nbsp;Between 9:37 AM and 3:23 PM PST we experienced increased error rates when running SQL queries and executing DDL statements in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>","service":"athena-us-east-1"},{"service_name":"AWS CloudFormation (N. Virginia)","summary":"[RESOLVED] Increased Error Rates","date":"1488311351","status":3,"details":"","description":"<div><span class=\"yellowfg\">11:50 AM PST</span>&nbsp;We can confirm elevated API error rates and failed stack operations for CloudFormation in the US-EAST-1 Region. </div><div><span class=\"yellowfg\"> 2:00 PM PST</span>&nbsp;We are seeing recovery for stack CREATE, UPDATE and DELETE operations in the US-EAST-1 Region. We will continue to monitor this issue until fully recovered in US-EAST-1.</div><div><span class=\"yellowfg\"> 4:06 PM PST</span>&nbsp;Between 9:37 AM and 3:43 PM PST, we experienced elevated API error rates and failures in stack create, update and delete operations for CloudFormation in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>","service":"cloudformation-us-east-1"},{"service_name":"Amazon Elastic MapReduce (N. Virginia)","summary":"[RESOLVED] Increased Error Rates","date":"1488311387","status":3,"details":"","description":"<div><span class=\"yellowfg\">11:50 AM PST</span>&nbsp;We can confirm errors launching new Amazon EMR clusters in the US-EAST-1 Region. We are also investigating errors when existing clusters are attempting to access data and code objects in Amazon S3.</div><div><span class=\"yellowfg\"> 1:58 PM PST</span>&nbsp;We are starting to see recovery when launching new clusters in the US-EAST-1. We are also starting to see recovery when accessing data and code objects in Amazon S3 from existing clusters in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 2:51 PM PST</span>&nbsp;We continue to see recovery when launching new clusters in the US-EAST-1 Region. We also continue to see recovery when accessing data and code objects in Amazon S3 from existing clusters.</div><div><span class=\"yellowfg\"> 5:30 PM PST</span>&nbsp;Between 9:37 AM and 5:20 PM PST we experienced increased error rates when launching new clusters in the US-EAST-1 Region and when existing clusters were accessing data and code objects in Amazon S3 in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>","service":"emr-us-east-1"},{"service_name":"Amazon Kinesis Firehose (N. Virginia)","summary":"[RESOLVED] Increased Error Rates","date":"1488311386","status":3,"details":"","description":"<div><span class=\"yellowfg\">11:50 AM PST</span>&nbsp;We can confirm increased error rates in delivering data to S3 in the US-EAST-1 Region</div><div><span class=\"yellowfg\"> 3:19 PM PST</span>&nbsp;While we are seeing recovery in delivering data to service destinations, we continue to see errors in processing data with Lambda in the US-EAST-1 Region. </div><div><span class=\"yellowfg\"> 4:18 PM PST</span>&nbsp;Between 9:37 AM and 4:15 PM PST we experienced increased error rates for the Kinesis Firehose APIs in the US-EAST-1 Region. The issue has been resolved and the service is operating normally. </div>","service":"firehose-us-east-1"},{"service_name":"Amazon WorkSpaces (N. Virginia)","summary":"[RESOLVED] Increased Error Rates","date":"1488311449","status":1,"details":"","description":"<div><span class=\"yellowfg\">11:51 AM PST</span>&nbsp;We can confirm increased error rates for launching new WorkSpaces in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 1:31 PM PST</span>&nbsp;We continue to experience elevated error rates for new WorkSpaces launches in the US-EAST-1 Region as well as elevated error rates for connections to existing WorkSpaces when using new or modified registration codes. Also, WorkSpaces backup snapshot completion is delayed for some WorkSpaces in the US-EAST-1-Region.</div><div><span class=\"yellowfg\"> 3:03 PM PST</span>&nbsp;Between 9:37 AM and 2:40 PM PST we experienced elevated error rates for WorkSpaces launches and rebuilds in the US-EAST-1 Region as well as elevated error rates for connections to existing WorkSpaces when using new or modified registration codes or connecting to AutoStop Workspaces. Also, WorkSpaces backup snapshot completion was delayed for some WorkSpaces in the US-EAST-1-Region. The issue has been resolved and the service is operating normally.</div>","service":"workspaces-us-east-1"},{"service_name":"Amazon AppStream 2.0 (N. Virginia)","summary":"[RESOLVED] Increased Error Rates","date":"1488311506","status":2,"details":"","description":"<div><span class=\"yellowfg\">11:52 AM PST</span>&nbsp;We can confirm increased error rates for new AppStream 2.0 fleet and stack launches in the US-EAST-1 Region. We are also experiencing increased error rates for connections to AppStream 2.0 stacks in the US-East-1 Region.</div><div><span class=\"yellowfg\"> 3:15 PM PST</span>&nbsp;Between 9:37 AM and 3:10 PM PST we experienced increased failure rates for new AppStream 2.0 fleet and stack launches in the US-EAST-1 Region. Connections to AppStream 2.0 stacks were also impacted. The issue has been resolved and the service is operating normally.</div>","service":"appstream2-us-east-1"},{"service_name":"Amazon API Gateway (N. Virginia)","summary":"[RESOLVED] Increased Error Rates","date":"1488311692","status":1,"details":"","description":"<div><span class=\"yellowfg\">11:56 AM PST</span>&nbsp;We have confirmed increased error rates for API Gateway requests in the US-EAST-1 Region when communicating with other AWS services. Deploying new APIs or modifications to existing APIs are also affected.</div><div><span class=\"yellowfg\"> 2:19 PM PST</span>&nbsp;Between 9:37 AM and 1:57 PM PST we experienced elevated error rates for API Gateway requests in the US-EAST-1 Region when communicating with other AWS services. Deploying new APIs or modifications to existing APIs was also affected. The issue has been resolved and the service is operating normally.</div>","service":"apigateway-us-east-1"},{"service_name":"Amazon Mobile Analytics (N. Virginia)","summary":"[RESOLVED] Increased Error Rates","date":"1488311453","status":2,"details":"","description":"<div><span class=\"yellowfg\">11:57 AM PST</span>&nbsp;We can confirm elevated error rates when exporting analytics event data in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 2:19 PM PST</span>&nbsp;We continue to see recovery of our Event Data Exports in Mobile Analytics in the US-EAST-1 Region. All other functions are operating normally. We have identified the root cause, and continue to work towards full recovery. </div><div><span class=\"yellowfg\"> 2:38 PM PST</span>&nbsp;Between 9:37 AM and 2:36 PM PST we experienced increased error rates for read/write operations in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>","service":"analytics-us-east-1"},{"service_name":"AWS Mobile Hub (N. Virginia)","summary":"[RESOLVED] Increased Error Rates","date":"1488311810","status":1,"details":"","description":"<div><span class=\"yellowfg\">11:57 AM PST</span>&nbsp;We can confirm impact affecting the delivery of sample applications and custom SDKs, as well as the creation of APIs in the Cloud Logic feature.</div><div><span class=\"yellowfg\"> 2:04 PM PST</span>&nbsp;Between 9:37 AM and 1:49 PM PST we experienced elevated failure rate for sample app and custom SDK generation and cloud logic API creation for all Mobile Hub projects. Other features were not impacted. The issue has been resolved and the service is operating normally. </div>","service":"mobilehub-us-east-1"},{"service_name":"Amazon Pinpoint (N. Virginia)","summary":"[RESOLVED] Increased Error Rates","date":"1488311836","status":2,"details":"","description":"<div><span class=\"yellowfg\">11:57 AM PST</span>&nbsp;We can confirm elevated error rates for our campaign and push notification APIs in the US-EAST-1 Region as well as elevated error rates on exporting analytics event data.</div><div><span class=\"yellowfg\"> 3:08 PM PST</span>&nbsp;We are seeing recovery of campaign notification API calls in the US-EAST-1 Region. All other functions are operating normally. We have identified the root cause, and continue to work towards full recovery.</div><div><span class=\"yellowfg\"> 4:31 PM PST</span>&nbsp;Between 9:37 AM PST and 4:26 PM PST we experienced elevated error rates for campaign notification API calls and event data export in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>","service":"pinpoint-us-east-1"},{"service_name":"Amazon CloudSearch (N. Virginia)","summary":"[RESOLVED] Increased Error Rates","date":"1488311828","status":2,"details":"","description":"<div><span class=\"yellowfg\">11:58 AM PST</span>&nbsp;We can confirm elevated API error rates when creating, deleting, and updating Domains in the US-EAST-1 Region. Document updates to existing domains are also experiencing elevated error rates.</div><div><span class=\"yellowfg\"> 2:28 PM PST</span>&nbsp;We are seeing recovery for creating, deleting, and updating Domains in the US-EAST-1 Region. Document updates to existing domains are now working. We are continuing to experience higher than normal latencies for creating new Domains. </div><div><span class=\"yellowfg\"> 4:11 PM PST</span>&nbsp;Between 9:37 AM and 1:43 PM PST, customers experienced elevated API error rates in creating, updating and deleting CloudSearch domains and document update API calls in the US-EAST-1 Region. Customers continued to experience delays in creating new domains until 3:55 PM PST. The issue has been resolved and the service is operating normally.</div>","service":"cloudsearch-us-east-1"},{"service_name":"AWS QuickSight (N. Virginia)","summary":"[RESOLVED] Increased Error Rates","date":"1488311838","status":1,"details":"","description":"<div><span class=\"yellowfg\">11:58 AM PST</span>&nbsp;We can confirm that ingestion into SPICE is experiencing increased error rates for Amazon QuickSight in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 2:03 PM PST</span>&nbsp;Between 9:37 AM and 1:58 PM PST we experienced increased error rates for ingestion into SPICE for QuickSight in the US-EAST-1 Region. Existing analyses and dashboards were not impacted. The issue has been resolved and the service is operating normally.</div>","service":"quicksight-us-east-1"},{"service_name":"Amazon Elasticsearch Service (N. Virginia)","summary":"[RESOLVED] Increased Error Rates","date":"1488311874","status":2,"details":"","description":"<div><span class=\"yellowfg\">11:58 AM PST</span>&nbsp;We can confirm elevated API error rates when creating, deleting, and updating Elasticsearch Domains in the US-EAST-1 Region. Connectivity to existing Domains is not affected. </div><div><span class=\"yellowfg\"> 2:34 PM PST</span>&nbsp;We are seeing recovery for creating, deleting, and updating Elasticsearch Domains in the US-EAST-1 Region. Connectivity to existing Domains is not affected. We are continuing to experience higher than normal latencies for creating new Domains.</div><div><span class=\"yellowfg\"> 4:08 PM PST</span>&nbsp;Between 9:37 AM and 1:43 PM PST, customers experienced elevated API error rates in creating, updating and deleting Elasticsearch domains in the US-EAST-1 Region. Customers continued to experience delays in creating new domains until 3:55 PM PST. The issue has been resolved and the service is operating normally. </div>","service":"elasticsearch-us-east-1"},{"service_name":"AWS Data Pipeline (N. Virginia)","summary":"[RESOLVED] Increased Error Rates","date":"1488311936","status":2,"details":"","description":"<div><span class=\"yellowfg\">12:02 PM PST</span>&nbsp;We can confirm errors launching new Amazon Data pipeline clusters in the US-EAST-1 Region. We are also investigating errors when existing clusters are attempting to access data and code objects in Amazon S3.</div><div><span class=\"yellowfg\"> 2:16 PM PST</span>&nbsp;We are starting to see recovery when launching new Amazon Data pipeline clusters in the US-EAST-1. We are also starting to see recovery when accessing data in Amazon S3 from existing clusters in the US-EAST-1 region.</div><div><span class=\"yellowfg\"> 5:34 PM PST</span>&nbsp;Between 9:37 AM and 5:18 PM PST we experienced increased error rates when launching new Amazon Data pipeline clusters in the US-EAST-1 Region and when existing clusters were accessing data in Amazon S3 in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>","service":"datapipeline-us-east-1"},{"service_name":"AWS Config (N. Virginia)","summary":"[RESOLVED] Increased Error Rates","date":"1488312098","status":1,"details":"","description":"<div><span class=\"yellowfg\">12:02 PM PST</span>&nbsp;We can confirm elevated latencies in processing of configuration changes and evaluation of Config Rules in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 6:12 PM PST</span>&nbsp;We are recording configuration changes and evaluating Config Rules at a reduced frequency in the US-EAST-1 Region as we process our backlog from the event earlier today. We continue to work towards full resolution. </div><div><span class=\"yellowfg\"> 8:16 PM PST</span>&nbsp;Between 9:37 AM and 8:10 PM PST, we experienced elevated latencies in processing of configuration changes and evaluation of Config Rules in the US-EAST-1 Region. The issue has been resolved and the service is operating normally. </div>","service":"config-us-east-1"},{"service_name":"Amazon EC2 Container Registry (N. Virginia)","summary":"[RESOLVED] Increased Error Rates","date":"1488311929","status":2,"details":"","description":"<div><span class=\"yellowfg\">12:02 PM PST</span>&nbsp;We can confirm increased error rates and elevated latencies pushing or pulling container images from Amazon ECR repositories in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 1:53 PM PST</span>&nbsp;We can confirm recovery of get APIs and pulling container images from Amazon ECR repositories in the US-EAST-1 Region. We continue to work toward recovery for write APIs and pushing container images.</div><div><span class=\"yellowfg\"> 2:13 PM PST</span>&nbsp;Between 9:37 AM and 1:51 PM PST we experienced increased error rates for APIs and elevated latencies pushing or pulling container images from Amazon ECR repositories in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>","service":"ecr-us-east-1"},{"service_name":"Amazon Cognito (N. Virginia)","summary":"[RESOLVED] Increased Error Rates","date":"1488312164","status":2,"details":"","description":"<div><span class=\"yellowfg\">12:03 PM PST</span>&nbsp;We can confirm increased error rate for Cognito Sync operations in the US-EAST-1 Region. Cognito User Pools and Cognito Identity are operating normally. We have identified the issue and working to restore normal operations.</div><div><span class=\"yellowfg\"> 1:11 PM PST</span>&nbsp;We can confirm increased error rate for Cognito Sync and Cognito User Pools operations in the US-EAST-1 Region. Cognito Identity is operating normally. We have identified the issue and working to restore normal operations.</div><div><span class=\"yellowfg\"> 1:48 PM PST</span>&nbsp;We continue to experience elevated error rates in Cognito Sync and user SignUp operations in Cognito User Pools. We have started to see recovery for all remaining operations for Cognito User Pools in the US-EAST-1 Region. We have identified the root cause and are working to remediate the issue. Cognito Identity is operating normally. </div><div><span class=\"yellowfg\"> 2:27 PM PST</span>&nbsp;We continue to experience elevated error rates for update operations in Cognito Sync. Cognito User Pools and Cognito Identity are operating normally. </div><div><span class=\"yellowfg\"> 2:59 PM PST</span>&nbsp;Between 9:37 AM and 2:55 PM PST, we experienced increased error rates for Cognito Sync and Cognito User Pools in the US-EAST-1 Region. The issue has been resolved and the service is operating normally. </div>","service":"cognito-us-east-1"},{"service_name":"Amazon ElastiCache (N. Virginia)","summary":"[RESOLVED] Increased Error Rates","date":"1488311909","status":1,"details":"","description":"<div><span class=\"yellowfg\">12:03 PM PST</span>&nbsp;We can confirm increased failure rates in node creates and modifications, and delays in node backups in the US-EAST-1 Region. A small number of customers are unable to access their nodes.</div><div><span class=\"yellowfg\"> 3:43 PM PST</span>&nbsp;Between 09:37 AM and 03:40 PM PST we experienced increased failure rates in node creates and modifications, and delays in node backups. The issue has been resolved and the service is operating normally.</div>","service":"elasticache-us-east-1"},{"service_name":"AWS Certificate Manager (N. Virginia)","summary":"[RESOLVED] Increased Error Rates","date":"1488312165","status":1,"details":"","description":"<div><span class=\"yellowfg\">12:03 PM PST</span>&nbsp;We have confirmed increased error rates for binding certificates and first time certificate issuance in the US-EAST-1 Region. We have also confirmed increased error rates regarding validation emails in all regions.</div><div><span class=\"yellowfg\"> 2:13 PM PST</span>&nbsp;Between 9:37 AM and 1:56 PM PST we experienced increased error rates for binding certificates and first time issuance in the US-EAST-1 Region. We also saw delays in validation e-mails in all regions. The issue has been resolved and the service is operating normally.</div>","service":"certificatemanager-us-east-1"},{"service_name":"Amazon Glacier (N. Virginia)","summary":"[RESOLVED] Increased Error Rates","date":"1488312126","status":2,"details":"","description":"<div><span class=\"yellowfg\">12:04 PM PST</span>&nbsp;We can confirm increased error rates for uploadArchive operations in the US-EAST-1 Region. </div><div><span class=\"yellowfg\"> 2:01 PM PST</span>&nbsp;Between 9:37 AM and 1:56 PM PST we experienced increased error rates for Glacier APIs and delayed archive retrievals. The issue has been resolved and the service is operating normally.</div>","service":"glacier-us-east-1"},{"service_name":"Amazon Lightsail (N. Virginia)","summary":"[RESOLVED] Increased Error Rates","date":"1488312216","status":2,"details":"","description":"<div><span class=\"yellowfg\">12:04 PM PST</span>&nbsp;We can confirm increased error rates for the Lightsail APIs and failures on creation of new Lightsail instances in the US-EAST-1 Region. New customers are unable to access the service.</div><div><span class=\"yellowfg\"> 2:51 PM PST</span>&nbsp;We are starting to see recovery for the Lightsail API and new instance creation in the US-EAST-1 Region. Some customers may still see \"Request Limit Exceeded\" errors until we are fully recovered.</div><div><span class=\"yellowfg\"> 3:53 PM PST</span>&nbsp;Between 9:37 AM and 2:55 PM PDT we experienced elevated error rates for the Lightsail API and failures creating new Lightsail instances. Connectivity to existing Lightsail instances was not affected. The issue has been resolved and the service is now operating normally.</div>","service":"lightsail-us-east-1"},{"service_name":"AWS CloudTrail (N. Virginia)","summary":"[RESOLVED] Increased Error Rates","date":"1488312304","status":1,"details":"","description":"<div><span class=\"yellowfg\">12:10 PM PST</span>&nbsp;We can confirm elevated latencies in delivering CloudTrail log files in the US-EAST-1 Region. </div><div><span class=\"yellowfg\"> 5:08 PM PST</span>&nbsp;We can confirm delivery latency improvements and have started backfilling CloudTrail log files in the US-EAST-1 Region. We continue to monitor latency until fully recovered.</div><div><span class=\"yellowfg\"> 8:14 PM PST</span>&nbsp;Between 9:37 AM and 8:10 PM PST we experienced issues in delivering CloudTrail log files to destinations in the US-EAST-1 Region. The issue has been resolved and the service is operating normally. Some customers may see delays in delivering log files from the impacted period of time as CloudTrail works through the backlog. </div>","service":"cloudtrail-us-east-1"},{"service_name":"Amazon Elastic Transcoder (N. Virginia)","summary":"[RESOLVED] Increased Error Rates","date":"1488315484","status":2,"details":"","description":"<div><span class=\"yellowfg\"> 1:09 PM PST</span>&nbsp;We can confirm delays starting encoding jobs in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 2:13 PM PST</span>&nbsp;Between 9:37 AM and 1:45 PM PST customers experienced delays starting encoding jobs in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>","service":"elastictranscoder-us-east-1"},{"service_name":"Amazon Inspector (N. Virginia)","summary":"[RESOLVED] Increased Error Rates","date":"1488312645","status":3,"details":"","description":"<div><span class=\"yellowfg\"> 1:09 PM PST</span>&nbsp;We can confirm increased API error rates in the US-EAST-1 Region. </div><div><span class=\"yellowfg\"> 2:42 PM PST</span>&nbsp;Between 9:37 AM and 1:45 PM PST we experienced increased API error rates and latencies in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>","service":"inspector-us-east-1"},{"service_name":"AWS CloudHSM (N. Virginia)","summary":"[RESOLVED] Increased Error Rates","date":"1488316154","status":1,"details":"","description":"<div><span class=\"yellowfg\"> 1:10 PM PST</span>&nbsp;We can confirm increased errors in creating, deleting and modifying CloudHSMs in the US-EAST-1 Region. Existing CloudHSMs are operating normally.</div><div><span class=\"yellowfg\"> 6:01 PM PST</span>&nbsp;Between 9:37 AM and 5:59 PM PST we experienced elevated error rates creating, modifying, and deleting HSMs in the US-EAST-1 Region. Connectivity to existing HSMs was not affected. The issue has been resolved and the service is operating normally. </div>","service":"cloudhsm-us-east-1"},{"service_name":"AWS Batch (N. Virginia)","summary":"[RESOLVED] Increased Error Rates","date":"1488312222","status":2,"details":"","description":"<div><span class=\"yellowfg\"> 1:15 PM PST</span>&nbsp;We are investigating increased API error rates and latencies for some AWS Batch APIs in the US-EAST-1 Region. </div><div><span class=\"yellowfg\"> 1:23 PM PST</span>&nbsp;We can confirm increased failure rates for APIs in the US-EAST-1 Region. AWS Batch Jobs pulling from S3 backed container registries are seeing a higher failure rate.</div><div><span class=\"yellowfg\"> 3:04 PM PST</span>&nbsp;We are continuing to see high failure rates for APIs in the US-EAST-1 Region. Existing Jobs in Job Queues are being processed.</div><div><span class=\"yellowfg\"> 3:50 PM PST</span>&nbsp;We are seeing recovery of API calls in the US-EAST-1 Region. We have identified the root cause, and continue to work towards full recovery.</div><div><span class=\"yellowfg\"> 5:01 PM PST</span>&nbsp;Between 9:37 AM and 4:45 PM PST we experienced high failures rates for APIs and elevated latencies creating, updating and deleting AWS Batch resources in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>","service":"batch-us-east-1"},{"service_name":"Amazon CloudWatch (N. Virginia)","summary":"[RESOLVED] Elevated API Faults ","date":"1488477603","status":1,"details":"","description":"<div><span class=\"yellowfg\">10:00 AM PST</span>&nbsp;We are investigating increased faults for CloudWatch alarms APIs and delays in processing some alarms in the US-EAST-1 Region.</div><div><span class=\"yellowfg\">10:36 AM PST</span>&nbsp;Between 9:29 AM and 10:00 AM PST, customers experienced increased faults for CloudWatch alarms APIs and delays in processing some alarms in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.\r\n</div>","service":"cloudwatch-us-east-1"},{"service_name":"Auto Scaling (N. Virginia)","summary":"[RESOLVED] Increased Error Rates","date":"1489005219","status":1,"details":"","description":"<div><span class=\"yellowfg\">12:33 PM PST</span>&nbsp;We are investigating increased error rates for API calls in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 1:14 PM PST</span>&nbsp;Between 12:01 PM and 1:03 PM PST, we experienced increased API faults and instance launch latencies in the US-EAST-1 Region. Existing instances were not impacted during the event. The issue has been resolved and the service is operating normally.</div>","service":"autoscaling-us-east-1"},{"service_name":"Amazon Route 53","summary":"[RESOLVED] ChangeResourceRecordSets Request Throttling","date":"1489448633","status":2,"details":"","description":"<div><span class=\"yellowfg\"> 4:44 PM PDT</span>&nbsp;We are investigating slow propagation of DNS edits to the Route 53 DNS servers. This does not impact queries to existing DNS records.</div><div><span class=\"yellowfg\"> 5:11 PM PDT</span>&nbsp;We continue to investigate slow propagation of DNS edits to the Route 53 DNS servers. This does not impact queries to existing DNS records.\r\n</div><div><span class=\"yellowfg\"> 6:34 PM PDT</span>&nbsp;We have identified root cause of the slow propagation of DNS edits to the Route 53 DNS servers and are working towards recovery. This does not impact queries to existing DNS records.</div><div><span class=\"yellowfg\"> 7:40 PM PDT</span>&nbsp;We continue to experience slow propagation times and continue to work towards full recovery. This does not impact queries to existing DNS records.</div><div><span class=\"yellowfg\">10:12 PM PDT</span>&nbsp;We continue to work on resolving the slow propagation times. Requests to the ChangeResourceRecordSets API are currently being throttled. Queries to existing DNS records remain unaffected.</div><div><span class=\"yellowfg\">Mar 14, 12:22 AM PDT</span>&nbsp;While changes are propagating, we continue to work through the backlog of pending changes that have accumulated. We expect full recovery to take several more hours. We have also throttled ChangeResourceRecordSets API call. Queries to existing DNS records remain unaffected</div><div><span class=\"yellowfg\">Mar 14,  1:40 AM PDT</span>&nbsp;Record changes are slowly propagating, while we work through the backlog of pending changes that have accumulated. We still expect full recovery to take several more hours. We are continuing to throttle ChangeResourceRecordSets API calls. Queries to existing DNS records remain unaffected.</div><div><span class=\"yellowfg\">Mar 14,  3:01 AM PDT</span>&nbsp;Record changes are still propagating, while we work through the backlog of pending changes that have accumulated. We expect full recovery to take several more hours. We are continuing to throttle ChangeResourceRecordSets API calls. Queries to existing DNS records remain unaffected.</div><div><span class=\"yellowfg\">Mar 14,  4:07 AM PDT</span>&nbsp;All outstanding DNS record changes have completed propagating. ChangeResourceRecordSets API calls are still being throttled. Queries to existing DNS records remain unaffected.</div><div><span class=\"yellowfg\">Mar 14,  5:12 AM PDT</span>&nbsp;ChangeResourceRecordSets API calls are still being throttled while we continue to recover. Queries to existing DNS records remain unaffected.</div><div><span class=\"yellowfg\">Mar 14,  7:12 AM PDT</span>&nbsp;We continue to throttle some ChangeResourceRecordSets API calls as we make progress towards recovery. Queries to existing DNS records remain unaffected.</div><div><span class=\"yellowfg\">Mar 14,  7:53 AM PDT</span>&nbsp;We are continuing to throttle some ChangeResourceRecordSets API calls while we work towards full recovery. Retries for throttled requests should succeed. Queries to existing DNS records remain unaffected.</div><div><span class=\"yellowfg\">Mar 14, 10:30 AM PDT</span>&nbsp;We continue to throttle some ChangeResourceRecordSets API calls while we make progress towards recovery. Retries for throttled requests should be successful. Queries to existing DNS records remain unaffected.</div><div><span class=\"yellowfg\">Mar 14,  1:11 PM PDT</span>&nbsp;We continue to remove throttling for the ChangeResourceRecordSets API as we continue towards recovery. At this stage, many customers are seeing recovery as DNS updates complete successful. For those customers that are still experiencing throttling, we continue to recommend retrying API requests or making use of change batches <a href=\"http://docs.aws.amazon.com/Route53/latest/APIReference/API_ChangeResourceRecordSets.html\">http://docs.aws.amazon.com/Route53/latest/APIReference/API_ChangeResourceRecordSets.html</a> to update multiple DNS records in a single request. Queries to existing DNS records remain unaffected.</div><div><span class=\"yellowfg\">Mar 14,  2:54 PM PDT</span>&nbsp;We have removed throttling for the ChangeResourceRecordSets API and are seeing recovery. All DNS update operations are now completing successfully. Queries to existing DNS records were not affected. The issue has been resolved and the service is operating normally.</div>","service":"route53"},{"service_name":"Amazon Elastic Compute Cloud (GovCloud)","summary":"[RESOLVED] Increased Instance Impairments","date":"1489786844","status":1,"details":"","description":"<div><span class=\"yellowfg\"> 2:40 PM PDT</span>&nbsp;Between 1:34 PM and 2:11 PM PDT we experienced impairments affecting some instances in the US-GOV-WEST-1 Region. The issue has been resolved and the service is operating normally. </div>","service":"ec2-us-gov-west-1"},{"service_name":"Amazon Elastic Compute Cloud (N. Virginia)","summary":"[RESOLVED] Increased API Error Rates","date":"1490211070","status":1,"details":"","description":"<div><span class=\"yellowfg\">12:31 PM PDT</span>&nbsp;We are investigating increased latencies for some EC2 instance launches in a single Availability Zone in the US-EAST-1 Region.</div><div><span class=\"yellowfg\">12:45 PM PDT</span>&nbsp;We have identified the underlying sub-system that is causing the increased instance launch latencies and delays in instance terminations in a single Availability Zone and are working to resolve the issue. We recommend that customers retry failed launches in the affected Availability Zone or launch in other Availability Zones within the region. This can be done by not specifying the Availability Zone in the launch request or targeting an unaffected Availability Zone.</div><div><span class=\"yellowfg\"> 1:13 PM PDT</span>&nbsp;We have taken action to address the issue with the sub-system that was causing the increased instance launch latencies and delays in instance terminations in a single Availability Zone. We are seeing recovery in instance launch times and termination error rates and continue to monitor the affected sub-system. Other Availability Zones remain unaffected by this issue.</div><div><span class=\"yellowfg\"> 1:30 PM PDT</span>&nbsp;Between 12:02 PM and 1:05 PM PDT we experienced an issue with a sub-system that resulted in increased instance launch latencies and delays in instance terminations in a single Availability Zone. Instance launches in other Availability Zones were unaffected. The issue has been resolved and the service is operating normally. </div>","service":"ec2-us-east-1"},{"service_name":"Amazon Elastic Compute Cloud (London)","summary":"[RESOLVED] t2.micro Instance Capacity","date":"1490373984","status":1,"details":"","description":"<div><span class=\"yellowfg\"> 9:46 AM PDT</span>&nbsp;We are temporarily running low on t2.micro instance capacity in the EU-WEST-2 Region. All other instance types are available. For customers that do receive an Insufficient Capacity Error for an instance launch request, we recommend using t2.nano, t2.small, t2.medium, t2.large or any of the other instance families. We are working to increase t2.micro instance capacity and expect to be back to normal levels within the next few hours. Instances that are currently running are not affected.</div><div><span class=\"yellowfg\">12:10 PM PDT</span>&nbsp;We have added additional t2.micro instance capacity in the EU-WEST-2 Region and are now successfully provisioning launches of new t2.micro instances. Until t2.micro instance capacity returns to normal levels, some customers may continue to see Insufficient Capacity Errors for new launch requests. In those cases, we continue to recommend using t2.nano, t2.small, t2.medium, t2.large or any of the other instance families. All other instance families (C4, D2, I3, M4, R4 and X1) and instances that are currently running remain unaffected. We expect t2.micro instance capacity to return to normal levels within the next hour.</div><div><span class=\"yellowfg\"> 1:08 PM PDT</span>&nbsp;We can confirm that t2.micro instance capacity has now returned to normal levels in the EU-WEST-2 Region and we are no longer returning Insufficient Capacity Errors for new t2.micro instance launch requests. The issue has been resolved and the service is operating normally. </div>","service":"ec2-eu-west-2"},{"service_name":"Auto Scaling (GovCloud)","summary":"[RESOLVED] Increased API errors","date":"1491570522","status":1,"details":"","description":"<div><span class=\"yellowfg\"> 6:08 AM PDT</span>&nbsp;We are investigating increased error rates for API calls in the US-GOV-WEST-1 Region.</div><div><span class=\"yellowfg\"> 6:30 AM PDT</span>&nbsp;Between 5:36 AM and 6:22 AM PDT we experienced elevated API error rates and delays in launching and terminating EC2 Instances managed by Auto Scaling in the US-GOV-WEST-1 Region. The issue has been resolved and the service is operating normally.</div>","service":"autoscaling-us-gov-west-1"},{"service_name":"AWS IoT (Oregon)","summary":"[RESOLVED] Increased API Error Rates","date":"1491591756","status":1,"details":"","description":"<div><span class=\"yellowfg\">12:02 PM PDT</span>&nbsp;Between 8:58 AM PDT and 11:35 AM PDT AWS IoT API calls to Rules engine, Thing Registry, and Security & Identity service experienced elevated error rates in the US-WEST-2 Region. MQTT message processing, Shadow operations, and existing rules in Rules engine were not affected during this time. The issue has been resolved and the service is operating normally.</div>","service":"awsiot-us-west-2"},{"service_name":"Amazon API Gateway (Oregon)","summary":"[RESOLVED] Elevated sigV4 Authentication Errors","date":"1491591992","status":1,"details":"","description":"<div><span class=\"yellowfg\">12:06 PM PDT</span>&nbsp;Between 8:58 AM PDT and 11:35 AM PDT Amazon API Gateway experienced elevated error rates for APIs configured to use SigV4 authentication in the US-WEST-2 Region. APIs using other authentication mechanisms, including custom authorizers and unauthenticated APIs, were not affected. The issue has been resolved and the service is operating normally.</div>","service":"apigateway-us-west-2"},{"service_name":"AWS CodeCommit (N. Virginia)","summary":"[RESOLVED] Increase Git Push/Pull Error Rates via SSH","date":"1491725642","status":1,"details":"","description":"<div><span class=\"yellowfg\"> 1:14 AM PDT</span>&nbsp;We are investigating increased Git Push/Pull error rates via the SSH protocol in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 1:58 AM PDT</span>&nbsp;Between 12:05 AM and 1:37 AM PDT we experienced increased Git Push/Pull error rates via the SSH protocol in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>","service":"codecommit-us-east-1"},{"service_name":"AWS CodeCommit (Oregon)","summary":"[RESOLVED] Increase Git Push/Pull Error Rates via SSH","date":"1491725729","status":1,"details":"","description":"<div><span class=\"yellowfg\"> 1:15 AM PDT</span>&nbsp;We are investigating increased Git Push/Pull error rates via the SSH protocol in the US-WEST-2 Region.</div><div><span class=\"yellowfg\"> 1:56 AM PDT</span>&nbsp;Between 12:05 AM and 1:37 AM PDT we experienced increased Git Push/Pull error rates via the SSH protocol in the US-WEST-2 Region. The issue has been resolved and the service is operating normally.</div>","service":"codecommit-us-west-2"},{"service_name":"AWS CodeCommit (Ireland)","summary":"[RESOLVED] Increase Git Push/Pull Error Rates via SSH","date":"1491725823","status":1,"details":"","description":"<div><span class=\"yellowfg\"> 1:17 AM PDT</span>&nbsp;We are investigating increased Git Push/Pull error rates via the SSH protocol in the EU-WEST-1 Region.</div><div><span class=\"yellowfg\"> 1:55 AM PDT</span>&nbsp;Between 12:05 AM and 1:37 AM PDT we experienced increased Git Push/Pull error rates via the SSH protocol in the EU-WEST-1 Region. The issue has been resolved and the service is operating normally.</div>","service":"codecommit-eu-west-1"},{"service_name":"AWS CodeCommit (Ohio)","summary":"[RESOLVED] Increase Git Push/Pull Error Rates via SSH","date":"1491725939","status":1,"details":"","description":"<div><span class=\"yellowfg\"> 1:19 AM PDT</span>&nbsp;We are investigating increased Git Push/Pull error rates via the SSH protocol in the US-EAST-2 Region.</div><div><span class=\"yellowfg\"> 1:52 AM PDT</span>&nbsp;Between 12:05 AM and 1:37 AM PDT we experienced increased Git Push/Pull error rates via the SSH protocol in the US-EAST-2 Region. The issue has been resolved and the service is operating normally.</div><div><span class=\"yellowfg\"> 1:56 AM PDT</span>&nbsp;</div>","service":"codecommit-us-east-2"},{"service_name":"AWS Lambda (N. Virginia)","summary":"[RESOLVED] Increased API Error Rates","date":"1491873439","status":1,"details":"","description":"<div><span class=\"yellowfg\"> 6:17 PM PDT</span>&nbsp;We are investigating increased API error rates in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 7:04 PM PDT</span>&nbsp;We can confirm increased API error rates in the US-EAST-1 Region and are continuing to investigate.</div><div><span class=\"yellowfg\"> 8:13 PM PDT</span>&nbsp;We continue to work towards full resolution. Customers may continue to experience increased latencies as we process accumulated events.</div><div><span class=\"yellowfg\"> 9:05 PM PDT</span>&nbsp;Between 5:25 PM and 9:00 PM PDT we experienced elevated error rates and event processing latencies in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>","service":"lambda-us-east-1"},{"service_name":"Amazon API Gateway (N. Virginia)","summary":"[RESOLVED] Elevated error rates for AWS Lambda integrations","date":"1491875387","status":1,"details":"","description":"<div><span class=\"yellowfg\"> 6:50 PM PDT</span>&nbsp;We are currently experiencing elevated error rates for APIs using Custom Authorizers and AWS Lambda integrations in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 7:40 PM PDT</span>&nbsp;We continue to investigate elevated error rates for APIs using Custom Authorizers and AWS Lambda integrations in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 8:23 PM PDT</span>&nbsp;We continue to work towards full resolution. Customers may experience intermittent errors in Custom Authorizers and AWS Lambda integrations in the US-EAST-1 Region. </div><div><span class=\"yellowfg\"> 9:09 PM PDT</span>&nbsp;Between 5:25 PM and 9:00 PM PDT customers using Custom Authorizers and AWS Lambda integrations in the US-EAST-1 Region experienced elevated error rates. The issue has been resolved and the service is operating normally.</div>","service":"apigateway-us-east-1"},{"service_name":"Amazon Pinpoint (N. Virginia)","summary":"[RESOLVED] Increased API Error rates","date":"1491877830","status":1,"details":"","description":"<div><span class=\"yellowfg\"> 7:30 PM PDT</span>&nbsp;We are investigating increased API error rates resulting in delayed execution of some campaigns in the US-EAST-1 Region. </div><div><span class=\"yellowfg\"> 8:30 PM PDT</span>&nbsp;Between 5:25 PM and 8:20 PM PDT we experienced elevated error rates for API calls resulting in failed execution of some campaigns in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>","service":"pinpoint-us-east-1"},{"service_name":"Amazon Athena (N. Virginia)","summary":"[RESOLVED] Increased Error Rates","date":"1492739814","status":1,"details":"","description":"<div><span class=\"yellowfg\"> 6:57 PM PDT</span>&nbsp;We are investigating increased error rates affecting a small number of customers running Amazon Athena queries in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 7:30 PM PDT</span>&nbsp;We have identified the root cause of the issue that was causing some customers to see increased error rates when running Amazon Athena queries in the US-EAST-1 Region and we are working towards recovery.</div><div><span class=\"yellowfg\"> 8:06 PM PDT</span>&nbsp;Between 6:00 PM and 7:50 PM PDT, we experienced increased error rates affecting a small number of customers running Amazon Athena queries in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>","service":"athena-us-east-1"},{"service_name":"Amazon CloudWatch (Oregon)","summary":"[RESOLVED] Elevated ListMetrics API faults","date":"1493032671","status":1,"details":"","description":"<div><span class=\"yellowfg\"> 4:18 AM PDT</span>&nbsp;We are investigating elevated fault rates for the CloudWatch ListMetrics API in the US-WEST-2 Region. During this customers may experience increased faults in CloudWatch Console</div><div><span class=\"yellowfg\"> 5:14 AM PDT</span>&nbsp;Between 3:28 AM and 5:00 AM PDT customers experienced elevated errors listing metrics via the CloudWatch ListMetrics API and the CloudWatch Management Console in the US-WEST-2 Region. The issue has been resolved and the service is operating normally.</div>","service":"cloudwatch-us-west-2"},{"service_name":"AWS Internet Connectivity (N. Virginia)","summary":"[RESOLVED] Network Connectivity","date":"1493106282","status":1,"details":"","description":"<div><span class=\"yellowfg\">12:44 AM PDT</span>&nbsp;Between 11:19 PM and 11:44 PM PDT we experienced connectivity issues between two Availability Zones in the US-EAST-1 Region. This issue has been resolved and all services are operating normally. </div>","service":"internetconnectivity-us-east-1"},{"service_name":"Amazon Relational Database Service (N. Virginia)","summary":"[RESOLVED] Increased Instance Create Latencies","date":"1493115807","status":1,"details":"","description":"<div><span class=\"yellowfg\"> 3:23 AM PDT</span>&nbsp;We are investigating increased database instance create times in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 4:31 AM PDT</span>&nbsp;Between 11:22 PM PDT 24th April and 04:06 AM PDT 25th April 2017, we experienced increased latencies for all database instance launches in the US-EAST-1 Region. Existing database instances were unaffected. The issue has been resolved and the service is operating normally.</div>","service":"rds-us-east-1"},{"service_name":"Amazon Elastic Compute Cloud (Ohio)","summary":"[RESOLVED] EBS Volume Performance","date":"1493167712","status":1,"details":"","description":"<div><span class=\"yellowfg\"> 5:48 PM PDT</span>&nbsp;We are investigating degraded performance for some EBS volumes in a Single Availability Zone in the US-EAST-2 Region.</div><div><span class=\"yellowfg\"> 6:36 PM PDT</span>&nbsp;Between 4:13 PM and 6:26 PM PDT, we experienced degraded performance for some EBS volumes in a single Availability Zone in the US-EAST-2 Region. The issue has been resolved and the service is operating normally.</div>","service":"ec2-us-east-2"},{"service_name":"Amazon Elastic Compute Cloud (GovCloud)","summary":"[RESOLVED] Increased API Error Rates","date":"1493321836","status":1,"details":"","description":"<div><span class=\"yellowfg\">12:37 PM PDT</span>&nbsp;We are investigating increased error rates for RunInstances API calls in the US-GOV-WEST-1 Region. Existing instances are unaffected. </div><div><span class=\"yellowfg\"> 1:28 PM PDT</span>&nbsp;Between 12:10 PM and 1:18 PM PDT we experienced increased error rates for RunInstances, Key Pairs and AMI related API calls in the US-GOV-WEST-1 Region. Existing running instances were unaffected. This issue has been resolved and the service is operating normally. </div>","service":"ec2-us-gov-west-1"},{"service_name":"AWS Identity and Access Management (N. Virginia)","summary":"[RESOLVED] Increased Error Rates and Latencies","date":"1493845393","status":1,"details":"","description":"<div><span class=\"yellowfg\"> 2:03 PM PDT</span>&nbsp;We are investigating increased error rates and latencies on IAM administrative APIs in US-EAST-1 Region. The IAM console and Create, Delete, List, Get, and Update API actions on IAM Users, Groups, Roles and Policies may be impacted. Other AWS services whose features require these actions will also be impacted. All existing authentications and authorizations are not impacted.</div><div><span class=\"yellowfg\"> 3:09 PM PDT</span>&nbsp;Between 1:01 PM and 2:48 PM PDT we experienced increased errors and latencies within the IAM console and Create, Delete, List, Get, and Update APIs. Other AWS services whose features require these actions were also impacted. The issue has been resolved and the service is now operating normally. </div>","service":"iam-us-east-1"},{"service_name":"AWS Certificate Manager (N. Virginia)","summary":"[RESOLVED] Increased Domain Validation Error Rates","date":"1493923583","status":2,"details":"","description":"<div><span class=\"yellowfg\">11:46 AM PDT</span>&nbsp;We are investigating increased error rates in domain validations in the US-EAST-1 Region. Customers are experiencing an error when attempting to approve certificate issuance on the validation website.</div><div><span class=\"yellowfg\">12:13 PM PDT</span>&nbsp;We can confirm customers are experiencing an error when attempting to validate a domain for a certificate request or renewal. This is preventing issuance of these certificates. Operations using existing certificates, including binding and unbinding certificates, are not impacted.</div><div><span class=\"yellowfg\">12:21 PM PDT</span>&nbsp;Between 8:55 AM PDT and 12:17 PM PDT customers were experiencing errors when attempting to validate a domain. Operations with existing certificates were not impacted. Binding, unbinding of certificates with AWS services were not impacted. </div>","service":"certificatemanager-us-east-1"},{"service_name":"Amazon Elastic Compute Cloud (N. Virginia)","summary":"[RESOLVED] Degraded Volume Performance for some EBS Volumes","date":"1493962963","status":1,"details":"","description":"<div><span class=\"yellowfg\">10:42 PM PDT</span>&nbsp;We are currently investigating degraded performance for some EBS volumes in a single Availability Zone in the US-EAST-1 Region.</div><div><span class=\"yellowfg\">11:11 PM PDT</span>&nbsp;Between 10:22 PM and 10:39 PM PDT we experienced degraded performance for a small number of EBS volumes in a Single Availability zone in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>","service":"ec2-us-east-1"},{"service_name":"AWS CodeCommit (N. Virginia)","summary":"[RESOLVED] Increased API Error Rates","date":"1494358909","status":1,"details":"","description":"<div><span class=\"yellowfg\">12:42 PM PDT</span>&nbsp;We are investigating elevated error rates for Git Push and Pull and API requests in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 1:03 PM PDT</span>&nbsp;Between 11:41 AM and 12:54 PM PDT we experienced increased error rates and latencies in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>","service":"codecommit-us-east-1"},{"service_name":"AWS Marketplace","summary":"[RESOLVED] Increased Error Rates","date":"1494542117","status":1,"details":"","description":"<div><span class=\"yellowfg\"> 3:35 PM PDT</span>&nbsp;Between 2:28 PM and 3:19 PM PDT customers may have experienced a 500 error page when accessing the AWS Marketplace Website. The issue has been resolved and the service is operating normally.</div>","service":"marketplace"},{"service_name":"Amazon Elastic Compute Cloud (N. California)","summary":"[RESOLVED] Increased API Error Rates","date":"1494897236","status":1,"details":"","description":"<div><span class=\"yellowfg\"> 6:14 PM PDT</span>&nbsp;\"Between 5:33 PM and 6:00 PM PDT we experienced increased error rates and latencies for EC2 APIs in the US-WEST-1 Region. The issue was caused by a sub-system that manages resource tagging. This affected API requests with associated resource tags. The issue has been resolved and the service is operating normally.\"</div>","service":"ec2-us-west-1"},{"service_name":"Amazon Simple Storage Service (Sao Paulo)","summary":"[RESOLVED] Console Error Rates","date":"1494970947","status":1,"details":"","description":"<div><span class=\"yellowfg\"> 2:42 PM PDT</span>&nbsp;Between 12:38 PM and 1:22 PM PDT, we experienced increased error rates to the S3 Management Console in the SA-EAST-1 Region. The S3 API and CLI were operating normally. The issue impacting console access has been resolved and the console is operating normally.</div>","service":"s3-sa-east-1"},{"service_name":"Amazon Elastic Compute Cloud (GovCloud)","summary":"[RESOLVED] Increased API Error Rates","date":"1495498937","status":1,"details":"","description":"<div><span class=\"yellowfg\"> 5:22 PM PDT</span>&nbsp;We are investigating increased API error rates for the EC2 APIs in the US-GOV-WEST-1 Region.</div><div><span class=\"yellowfg\"> 5:38 PM PDT</span>&nbsp;We have identified the subsystem causing increased error rates for the EC2 APIs in the US-GOV-WEST-1 Region and are working to resolve the issue. Connectivity to existing instances is not affected.</div><div><span class=\"yellowfg\"> 5:51 PM PDT</span>&nbsp;Between 4:10 PM and 5:41 PM PDT we experienced periods of increased error rates for the EC2 APIs in the US-GOV-WEST-1 Region. The issue has been resolved and the service is operating normally.</div>","service":"ec2-us-gov-west-1"},{"service_name":"Amazon Elastic Compute Cloud (Tokyo)","summary":"[解決済] APIのエラー率の増加 | [RESOLVED] API Increased API Error Rates","date":"1496642272","status":1,"details":"","description":"<div><span class=\"yellowfg\">10:58 PM PDT</span>&nbsp;日本時間 午後 2:16 から 午後 2:40 の間、AP-NORTHEAST-1 (東京リージョン)の一つのアベイラビリティーゾーンにおいてEC2 APIのエラー率が増加いたしました。現在は解決しており通常通りご利用いただけます。 | Between 10:16 PM and 10:40 PM PDT we experienced increased error rates for the EC2 APIs in a single Availability Zone in the AP-NORTHEAST-1 Region. The issue has been resolved and the service is operating normally.</div>","service":"ec2-ap-northeast-1"},{"service_name":"Amazon Elastic Compute Cloud (Mumbai)","summary":"[RESOLVED] Degraded EBS Volume Performance","date":"1496793072","status":1,"details":"","description":"<div><span class=\"yellowfg\"> 4:51 PM PDT</span>&nbsp;We are investigating increased API error rates and degraded performance for some EBS volumes in a single Availability Zone in the AP-SOUTH-1 Region.</div><div><span class=\"yellowfg\"> 5:26 PM PDT</span>&nbsp;We are still investigating increased API error rates and degraded performance for some EBS volumes in a single Availability Zone in the AP-SOUTH-1 Region.</div><div><span class=\"yellowfg\"> 6:58 PM PDT</span>&nbsp;We have identified the cause of the increased API error rates and degraded performance for some EBS volumes in a single Availability Zone in the AP-SOUTH-1 Region and continue working towards resolution.</div><div><span class=\"yellowfg\"> 8:57 PM PDT</span>&nbsp;Between 4:28 PM and 6:35 PM PDT we experienced increased EBS API error rates and degraded performance for a small number of EBS volumes in a single Availability Zone in the AP-SOUTH-1 Region. The issue has been resolved and the service is operating normally. </div>","service":"ec2-ap-south-1"},{"service_name":"Amazon Relational Database Service (Mumbai)","summary":"[RESOLVED] Database Instance Unavailability","date":"1496798326","status":1,"details":"","description":"<div><span class=\"yellowfg\"> 6:18 PM PDT</span>&nbsp;We can confirm that some database instances in the AP-SOUTH-1 Region are experiencing degraded performance or are unavailable. We are working to resolve the issue.</div><div><span class=\"yellowfg\"> 7:22 PM PDT</span>&nbsp;We are seeing recovery for the majority of database instances that experienced degraded performance or unavailability in a single Availability Zone in the AP-SOUTH-1 Region. We are continuing to work to resolve the small number of instances still experiencing degraded performance or unavailability.</div><div><span class=\"yellowfg\"> 9:53 PM PDT</span>&nbsp;Between 4:26 PM and 6:35 PM PDT, some database instances experienced degraded performance or unavailability in a single Availability Zone in the AP-SOUTH-1 Region. The issue has been resolved and the service is operating normally. We are continuing to resolve the degraded performance or unavailability for a very small number of database instances and will update those customers through the Personal Health Dashboard.</div>","service":"rds-ap-south-1"},{"service_name":"Amazon Elastic Compute Cloud (N. Virginia)","summary":"[RESOLVED] Increased Launch Error Rates","date":"1497527076","status":1,"details":"","description":"<div><span class=\"yellowfg\"> 4:44 AM PDT</span>&nbsp;We are investigating increased error rates for new launches in a single Availability Zone in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 4:59 AM PDT</span>&nbsp;Between 4:11 AM and 4:45 AM PDT we experienced a small increase in the number of newly-launched instances that transitioned directly from a 'pending' state to 'terminated' state in a single Availability Zone in the US-EAST-1 Region. Existing running instances were unaffected. This issue has been resolved and the service is operating normally.</div>","service":"ec2-us-east-1"},{"service_name":"Amazon CloudFront","summary":"[RESOLVED] Elevated SSL Handshake Failures","date":"1497558990","status":1,"details":"","description":"<div><span class=\"yellowfg\"> 1:36 PM PDT</span>&nbsp;We’re investigating elevated SSL handshake failures from our CloudFront edge locations.</div><div><span class=\"yellowfg\"> 2:10 PM PDT</span>&nbsp;We have identified the cause of elevated SSL handshake failures from our CloudFront edge locations and are in the process of mitigating the issue.</div><div><span class=\"yellowfg\"> 2:58 PM PDT</span>&nbsp;We have identified the cause of elevated SSL handshake and BadRequest errors from our CloudFront edge locations and continue to work towards resolution.</div><div><span class=\"yellowfg\"> 3:45 PM PDT</span>&nbsp;Between 1:20 PM and 3:30 PM PDT, we experienced SSL Handshake and BadRequest errors to some CloudFront edge locations. The issue has been resolved and the service is operating normally.</div>","service":"cloudfront"},{"service_name":"Amazon WorkSpaces (Singapore)","summary":"[RESOLVED] Increased Error Rates","date":"1497562481","status":1,"details":"","description":"<div><span class=\"yellowfg\"> 2:34 PM PDT</span>&nbsp; We are investigating increased error rates for new connections to WorkSpaces in the AP-SOUTHEAST-1 Region. Existing connections are not impacted.</div><div><span class=\"yellowfg\"> 3:04 PM PDT</span>&nbsp;We continue to investigate increased error rates for new connections to WorkSpaces in the AP-SOUTHEAST-1 Region. Existing connections are not impacted.</div><div><span class=\"yellowfg\"> 3:18 PM PDT</span>&nbsp;Between 12:00 PM and 2:50 PM PDT, we experienced increased error rates for new connections to WorkSpaces in the AP-SOUTHEAST-1 Region. Existing connections were not affected. The issue has been resolved and the service is operating normally.\r\n</div>","service":"workspaces-ap-southeast-1"},{"service_name":"Amazon Elastic Compute Cloud (N. Virginia)","summary":"[RESOLVED] Increased Spot Instance Launch Delays","date":"1497908945","status":1,"details":"","description":"<div><span class=\"yellowfg\"> 2:49 PM PDT</span>&nbsp;We are investigating increased Spot Instance launch delays in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 3:39 PM PDT</span>&nbsp;We have identified and resolved the root cause of the increased Spot Instance launch delays in the US-EAST-1 Region. We are currently processing the backlog of launch requests as we continue to work toward full recovery.</div><div><span class=\"yellowfg\"> 4:54 PM PDT</span>&nbsp;Between 12:55 PM and 4:46 PM PDT we experienced increased Spot Instance launch delays in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>","service":"ec2-us-east-1"},{"service_name":"Amazon CloudWatch (Ireland)","summary":"[RESOLVED] Delayed Metrics","date":"1497985158","status":1,"details":"","description":"<div><span class=\"yellowfg\">11:59 AM PDT</span>&nbsp;We are investigating increased delays for CloudWatch metrics in the EU-WEST-1 Region. CloudWatch alarms may transition into \"INSUFFICIENT_DATA\" state if set on delayed metrics.</div><div><span class=\"yellowfg\">12:20 PM PDT</span>&nbsp;We can confirm increased delays for CloudWatch ELB metrics in the EU-WEST-1 Region. CloudWatch alarms may transition into \"INSUFFICIENT_DATA\" state if set on delayed metrics.</div><div><span class=\"yellowfg\">12:34 PM PDT</span>&nbsp;Between 9:55 AM and 12:25 PM PDT, some customers may have experienced intermittent delays in the availability of CloudWatch ELB metrics in the EU-WEST-1 Region. The issue has been resolved and the service is operating normally.</div>","service":"cloudwatch-eu-west-1"},{"service_name":"Amazon Elastic Load Balancing (Ireland)","summary":"[RESOLVED] Delayed CloudWatch Metrics","date":"1497987390","status":1,"details":"","description":"<div><span class=\"yellowfg\">12:36 PM PDT</span>&nbsp;Between 9:55 AM and 12:25 PM PDT, customers experienced intermittent delays in the availability of CloudWatch metrics for load balancers in the EU-WEST-1 Region. Connectivity to load balancers was not affected. The issue has been resolved and the service is operating normally.</div>","service":"elb-eu-west-1"},{"service_name":"Amazon CloudWatch (Ireland)","summary":"[RESOLVED] Elevated API faults and latencies in EU-WEST-1","date":"1498051081","status":1,"details":"","description":"<div><span class=\"yellowfg\"> 6:18 AM PDT</span>&nbsp;We are investigating increased faults and latencies for CloudWatch APIs and metrics in the EU-WEST-1 Region. CloudWatch alarms may transition into &quot;INSUFFICIENT_DATA&quot; state if set on delayed metrics.</div><div><span class=\"yellowfg\"> 6:43 AM PDT</span>&nbsp;Between 5:15 AM PDT and 6:31 AM PDT, some customers experienced elevated faults when calling CloudWatch APIs in the EU-WEST-1 Region. Some metrics were delayed, and CloudWatch alarms on delayed metrics transitioned into INSUFFICIENT_DATA state. Delayed metrics are in the process of backfilling in CloudWatch console graphs and for API retrieval. The issue has been resolved and the service is operating normally.</div>","service":"cloudwatch-eu-west-1"},{"service_name":"AWS Lambda (N. Virginia)","summary":"[RESOLVED] Increased API Error Rates and Latencies","date":"1498180054","status":3,"details":"","description":"<div><span class=\"yellowfg\"> 6:07 PM PDT</span>&nbsp;We are investigating increased API error rates and latencies in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 6:23 PM PDT</span>&nbsp;We can confirm increased API error rates and latencies in the US-EAST-1 Region</div><div><span class=\"yellowfg\"> 6:30 PM PDT</span>&nbsp;We can confirm increased API error rates and latencies in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 7:47 PM PDT</span>&nbsp;We are investigating issues in AWS Lambda&#x27;s capacity management subsystem that are causing increased API error rates and latencies in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 8:48 PM PDT</span>&nbsp;We have identified issues in AWS Lambda&#x27;s capacity subsystem related to the increased API error rates and latencies in the US-EAST-1 Region and continue to work toward resolution.</div><div><span class=\"yellowfg\"> 9:50 PM PDT</span>&nbsp;As we work toward resolution we are temporarily suspending traffic to US-EAST-1 to restore capacity. Previously accepted events will be processed as soon as service is fully restored.</div><div><span class=\"yellowfg\">10:25 PM PDT</span>&nbsp;We have restored capacity and are processing Lambda invocations. API error rates and latencies remain temporarily elevated in the US-EAST-1 Region as we complete our resolution steps.</div><div><span class=\"yellowfg\">11:39 PM PDT</span>&nbsp;We have restored capacity and are processing Lambda invocations. Latencies for some asynchronous invocations may be temporarily elevated in the US-EAST-1 Region as we work through the backlog of events.</div><div><span class=\"yellowfg\">Jun 23,  1:26 AM PDT</span>&nbsp;We are currently experiencing elevated latencies for some asynchronus invocations in the US-EAST-1 Region and continue to work towards resolution.</div><div><span class=\"yellowfg\">Jun 23,  3:05 AM PDT</span>&nbsp;Between June 22 5:31 PM and June 23 2:32 AM PDT, we experienced increased error rates for APIs and increased latencies in the US-EAST-1 Region related to Lambda's capacity subsystem. The issue has been resolved and the service is operating normally.</div>","service":"lambda-us-east-1"},{"service_name":"Amazon Lex (N. Virginia)","summary":"[RESOLVED] Increased API Error Rates ","date":"1498181084","status":1,"details":"","description":"<div><span class=\"yellowfg\"> 6:24 PM PDT</span>&nbsp;We are investigating increased API error rates to Lambda invocations from Amazon Lex in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 7:31 PM PDT</span>&nbsp;We continue to investigate increased API error rates to Lambda invocation from Amazon Lex in the US-EAST-1 Region</div><div><span class=\"yellowfg\"> 8:23 PM PDT</span>&nbsp;We can confirm increased error rates for AWS Lambda calls from Amazon Lex bots in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 9:39 PM PDT</span>&nbsp;We continue to experience increased error rates for AWS Lambda calls from Amazon Lex bots in the US-EAST-1 Region and are working towards a resolution.</div><div><span class=\"yellowfg\">11:19 PM PDT</span>&nbsp;We are beginning to see reduced error rates for AWS Lambda calls from Amazon Lex bots in the US-EAST-1 Region. We continue to work towards a full recovery.</div><div><span class=\"yellowfg\">Jun 23, 12:32 AM PDT</span>&nbsp;Between 5:31 PM PDT and 11:50 PM PDT we experienced increased error rates for AWS Lambda calls from Amazon Lex bots in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>","service":"lex-us-east-1"},{"service_name":"Amazon API Gateway (N. Virginia)","summary":"[RESOLVED] Increased Error Rates","date":"1498181169","status":2,"details":"","description":"<div><span class=\"yellowfg\"> 6:26 PM PDT</span>&nbsp;\r\nWe are investigating increased error rates for Lambda integrations and Custom Authorizers in the US-EAST-1 Region. </div><div><span class=\"yellowfg\"> 7:09 PM PDT</span>&nbsp;We continue to investigate increased error rates for Lambda integrations and Custom Authorizers in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 7:26 PM PDT</span>&nbsp;We continue to investigate increased error rates for Lambda integrations and Custom Authorizers in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 9:53 PM PDT</span>&nbsp;AWS Lambda integrations and Custom Authorizers will be temporarily unavailable in the US-EAST-1 Region as Lambda works towards resolution.</div><div><span class=\"yellowfg\">11:34 PM PDT</span>&nbsp;We are starting to see reduced error rates for Lambda integrations and Custom Authorizers in the US-EAST-1 Region.</div><div><span class=\"yellowfg\">Jun 23,  1:12 AM PDT</span>&nbsp;On June 22, between 5:31 PM and 11:17 PM PDT we experienced increased error rates for APIs with AWS Lambda integrations or Custom Authorizers in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>","service":"apigateway-us-east-1"},{"service_name":"AWS Step Functions (N. Virginia)","summary":"[RESOLVED] Increased State Transition Delays and Faults","date":"1498181190","status":2,"details":"","description":"<div><span class=\"yellowfg\"> 6:26 PM PDT</span>&nbsp;\r\nWe are investigating state transition delays and faults for Lambda steps in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 7:11 PM PDT</span>&nbsp;We continue to investigate state transition delays and faults for Lambda steps in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 7:56 PM PDT</span>&nbsp;We confirm increased state transition delays and faults for Lambda steps in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 9:36 PM PDT</span>&nbsp;We continue to experience state transition delays and faults for Lambda steps in the US-EAST-1 Region.</div><div><span class=\"yellowfg\">10:52 PM PDT</span>&nbsp;We can confirm that Step Functions with Lambda steps are now progressing in the US-EAST-1 Region.</div><div><span class=\"yellowfg\">Jun 23, 12:03 AM PDT</span>&nbsp;Between 5:31 PM and 11:50 PM PDT Step Functions customers with Lambda steps experienced state transition delays and faults in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>","service":"state-us-east-1"},{"service_name":"AWS Batch (N. Virginia)","summary":"[RESOLVED] Increased Error Rates","date":"1498181520","status":2,"details":"","description":"<div><span class=\"yellowfg\"> 6:32 PM PDT</span>&nbsp;We are investigating increased error rates in the US-EAST-1 Region. New job submission is impacted at this time. Existing jobs continue to execute.</div><div><span class=\"yellowfg\"> 7:10 PM PDT</span>&nbsp;We can confirm increased error rates in the US-EAST-1 Region. New job submission is impacted at this time. Existing jobs continue to execute.</div><div><span class=\"yellowfg\"> 8:02 PM PDT</span>&nbsp;We can confirm increased error rates in the Lambda Job intake queue for the US-EAST-1 Region. New job submission is impacted at this time. Existing jobs will continue to execute.</div><div><span class=\"yellowfg\">10:01 PM PDT</span>&nbsp;We continue to see high error rates in the Lambda Job intake queue for the US-EAST-1 Region. New job submission is impacted at this time. Existing jobs will continue to execute</div><div><span class=\"yellowfg\">11:00 PM PDT</span>&nbsp;We are beginning to see recovery and lower error rates in the submission of new jobs for the US-EAST-1 Region. New job submission is recovering. Existing jobs will continue to execute.</div><div><span class=\"yellowfg\">Jun 23,  1:06 AM PDT</span>&nbsp;Between June 22 5:31 PM and June 23 1:01 AM PDT we experienced increased error rates in the submission of new jobs for the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>","service":"batch-us-east-1"},{"service_name":"Amazon Cognito (N. Virginia)","summary":"[RESOLVED] Increased Error Rates","date":"1498182372","status":1,"details":"","description":"<div><span class=\"yellowfg\"> 6:46 PM PDT</span>&nbsp;We are seeing increased error rates for Lambda integrations for Cognito User Pools and Cognito Sync in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 7:52 PM PDT</span>&nbsp;We continue to see errors from Lambda Integrations for Cognito Sync and Cognito User Pools in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 9:05 PM PDT</span>&nbsp;We continue to see errors from Lambda Integrations for Cognito User Pools however we are seeing recovery for errors from Lambda Integrations for Cognito Sync in the US-EAST-1 Region.</div><div><span class=\"yellowfg\">11:36 PM PDT</span>&nbsp;We are seeing recovery for errors from Lambda Integrations for Cognito User Pools and Cognito Sync in the US-EAST-1 Region. </div><div><span class=\"yellowfg\">11:59 PM PDT</span>&nbsp;Between 5:31 PM and 11:40 PM PDT we experienced increased error rates for Lambda Invocations for Cognito Sync and Cognito User Pools in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>","service":"cognito-us-east-1"},{"service_name":"AWS Config (N. Virginia)","summary":"[RESOLVED] Increased Error Rates","date":"1498182902","status":1,"details":"","description":"<div><span class=\"yellowfg\"> 6:55 PM PDT</span>&nbsp;We can confirm increased error rates in execution of Config rules in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 8:05 PM PDT</span>&nbsp;We can confirm increased error rates and latencies in the Lambda execution of Config rules in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 9:53 PM PDT</span>&nbsp;We continue to see increased latencies and errors in the Lambda execution of Config rules and corresponding staleness in resource compliance results in the US-EAST-1 Region. </div><div><span class=\"yellowfg\">11:40 PM PDT</span>&nbsp;We are starting to see recovery and lower error rates and latencies in the Lambda execution of AWS Config rules for the US-EAST-1 Region.</div><div><span class=\"yellowfg\">Jun 23, 12:06 AM PDT</span>&nbsp;Between 5:31 PM and 11:46 PM PDT we experienced increased errors and latencies in the Lambda execution of AWS Config rules and corresponding staleness in resource compliance results in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>","service":"config-us-east-1"},{"service_name":"Amazon Pinpoint (N. Virginia)","summary":"[RESOLVED] Increased Error Rates","date":"1498183639","status":1,"details":"","description":"<div><span class=\"yellowfg\"> 7:07 PM PDT</span>&nbsp;We are investigating increased error rates and retries in execution of API calls in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 8:16 PM PDT</span>&nbsp;We can confirm increased error rates and latencies of API calls in the US-EAST-1 Region. We are working to restore normal operations.</div><div><span class=\"yellowfg\">11:44 PM PDT</span>&nbsp;Between 5:31 PM and 11:35 PM PDT we experienced an increased error rate for the Pinpoint API in the US-EAST-1 Region. The issue has been resolved and the service is operating normally. </div>","service":"pinpoint-us-east-1"},{"service_name":"Amazon Connect (N. Virginia)","summary":"[RESOLVED] Increased error rates for Lambda integrated contact flows","date":"1498187508","status":1,"details":"","description":"<div><span class=\"yellowfg\"> 8:12 PM PDT</span>&nbsp;We are seeing increased error rates on contact flows that access Lambda functions in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 9:30 PM PDT</span>&nbsp;We are continuing to see error rates on contact flows that access Lambda functions in the US-EAST-1 Region.</div><div><span class=\"yellowfg\">11:56 PM PDT</span>&nbsp;Between 5:31 PM and 10:35 PM PDT we experienced increased error rates on contact flows that access Lambda functions in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>","service":"connect-us-east-1"},{"service_name":"AWS Greengrass (N. Virginia)","summary":"[RESOLVED] Increased API Error Rates","date":"1498187710","status":2,"details":"","description":"<div><span class=\"yellowfg\"> 8:15 PM PDT</span>&nbsp;We are experiencing increased error rates for our APIs in the US-EAST-1 Region and are working to resolve the issue.</div><div><span class=\"yellowfg\">10:01 PM PDT</span>&nbsp;We continue to experience increased error rates for our APIs in the US-EAST-1 Region and are working towards a resolution.</div><div><span class=\"yellowfg\">Jun 23, 12:58 AM PDT</span>&nbsp;We have identified the root cause of increased error rates for our APIs in the US-EAST-1 Region and continue to work towards resolution.</div><div><span class=\"yellowfg\">Jun 23,  2:29 AM PDT</span>&nbsp;We are beginning to see recovery on increased error rates for our APIs in the US-EAST-1 Region and continue to work towards full resolution.</div><div><span class=\"yellowfg\">Jun 23,  3:05 AM PDT</span>&nbsp;Between June 22 5:31 PM and June 23 2:45 AM PDT, we experienced increased error rates for APIs in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>","service":"awsgreengrass-us-east-1"},{"service_name":"Amazon Route 53","summary":"[RESOLVED] Increased API Error Rates","date":"1498515834","status":1,"details":"","description":"<div><span class=\"yellowfg\"> 3:25 PM PDT</span>&nbsp;Between 2:44 PM and 3:05 PM PDT we experienced increased error rates impacting API operations for DNS changes. This did not impact DNS queries for existing DNS records. The issue has been resolved and the service is operating normally.</div>","service":"route53"},{"service_name":"AWS CodeDeploy (N. Virginia)","summary":"[RESOLVED] Increased Deployment Error Rates","date":"1499288867","status":1,"details":"","description":"<div><span class=\"yellowfg\"> 2:07 PM PDT</span>&nbsp;Starting on June 30th 2:20 PM PDT AWS CodeDeploy released an update to the AWS CodeDeploy agent that prevents agents on some Linux instances from polling for commands in the US-EAST-1 Region. The AWS CodeDeploy team is actively working on a fix that will eliminate the need for manual actions on your behalf. In the meantime, if you are experiencing deployment timeout errors, you can fix the issue by <a href=\"http://docs.aws.amazon.com/codedeploy/latest/userguide/codedeploy-agent-operations-uninstall.html\">uninstalling the CodeDeploy agent</a>, and then <a href=\"http://docs.aws.amazon.com/codedeploy/latest/userguide/codedeploy-agent-operations-install.html\">reinstalling the CodeDeploy agent</a>.</div><div><span class=\"yellowfg\"> 6:02 PM PDT</span>&nbsp;We have identified the issue that is causing agents to fail to poll for commands. We have developed a fix and are in the process of validating and deploying the change. In the meantime, if you are experiencing deployment timeout errors, you can fix the issue by <a href=\"http://docs.aws.amazon.com/codedeploy/latest/userguide/codedeploy-agent-operations-uninstall.html\">uninstalling the CodeDeploy agent</a> and then <a href=\"http://docs.aws.amazon.com/codedeploy/latest/userguide/codedeploy-agent-operations-install.html\">reinstalling the CodeDeploy agent</a>.</div><div><span class=\"yellowfg\"> 9:27 PM PDT</span>&nbsp;Between June 30th 2:20 PM and July 5th 9:00 PM PDT AWS CodeDeploy experienced increased deployment error rates in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>","service":"codedeploy-us-east-1"},{"service_name":"Amazon Route 53 Domain Registration","summary":" [RESOLVED] Domain Registration Nameserver Configuration Errors","date":"1499434604","status":1,"details":"","description":"<div><span class=\"yellowfg\"> 6:36 AM PDT</span>&nbsp;We are investigating reports that domain nameservers may unintentionally change for domains registered under .ch, .com.au, .co.nz, .es, .im, .it, .jp, .net.au, .nl, .nz, .se and .sh, which are registered through our third party registrar partner. If your domain is affected, we suggest using the Route 53 console to re-apply your nameserver settings to your domains. DNS Queries to Route 53 DNS are not affected by this issue.</div><div><span class=\"yellowfg\"> 7:10 AM PDT</span>&nbsp;We have confirmed with our third party registrar provider that they have an issue in their system that changed nameservers for domains registered in .ch, .com.au, .co.nz, .es, .im, .it, .jp, .net.au, .nl, .nz, .se and .sh. They are working to identify all affected domains and backfill those nameservers to point to the correct nameservers. In the meantime, we suggest using the Route 53 console to re-apply your nameserver settings to your domains. DNS Queries to Route 53 DNS are not affected by this issue.</div><div><span class=\"yellowfg\"> 7:31 AM PDT</span>&nbsp;Our third party registrar provider has corrected all nameserver entries for the affected domains registered under .ch, .com.au, .co.nz, .es, .im, .it, .jp, .net.au, .nl, .nz, .se and .sh. DNS Queries to Route 53 DNS were not affected by this issue. The issue has been resolved and the service is operating normally. </div>","service":"route53domainregistration"},{"service_name":"Amazon Kinesis Firehose (N. Virginia)","summary":"[RESOLVED] Increased API Error Rates ","date":"1499884277","status":2,"details":"","description":"<div><span class=\"yellowfg\">11:31 AM PDT</span>&nbsp;We are investigating increased API fault rates in the US-EAST-1 Region</div><div><span class=\"yellowfg\">11:49 AM PDT</span>&nbsp;We continue to investigate increased API fault rates, specifically PUTs, in the US-EAST-1 Region.</div><div><span class=\"yellowfg\">12:06 PM PDT</span>&nbsp;Between 10:51 AM and 11:55 AM PDT we experienced increased API fault rates in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>","service":"firehose-us-east-1"},{"service_name":"Amazon EC2 Systems Manager (N. California)","summary":"[RESOLVED] Increased API Error Rates","date":"1500408917","status":1,"details":"","description":"<div><span class=\"yellowfg\"> 1:17 PM PDT</span>&nbsp;We are investigating increased API error rates in the US-WEST-1 Region.</div><div><span class=\"yellowfg\"> 1:54 PM PDT</span>&nbsp;We can confirm that we are experiencing increased API error rates in the US-WEST-1 Region, and we continue to work towards resolution.</div><div><span class=\"yellowfg\"> 2:42 PM PDT</span>&nbsp;We have identified the root cause of the increased API error rates in the US-WEST-1 Region and continue to work toward resolution.</div><div><span class=\"yellowfg\"> 3:21 PM PDT</span>&nbsp;Between 11:35 AM and 3:15 PM PDT we experienced increased API error rates in the US-WEST-1 Region. The issue has been resolved and the service is operating normally.</div>","service":"ec2systemsmanager-us-west-1"},{"service_name":"Amazon Elastic Compute Cloud (N. Virginia)","summary":"[RESOLVED] Increased Launch Failure Rates ","date":"1500710269","status":"1","details":"","description":"<div><span class=\"yellowfg\">12:58 AM PDT</span>&nbsp;We are investigating increased error rates for (some) new launches in a single Availability Zone in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 1:16 AM PDT</span>&nbsp;Between July 21 11:45 PM and July 22 12:56 AM PDT we experienced increased error rates for instance launches in the US-EAST-1 Region. Existing instances were unaffected. The issue has been resolved and the service is operating normally.</div>","service":"ec2-us-east-1"},{"service_name":"AWS Internet Connectivity (Sao Paulo)","summary":"[RESOLVED] Internet Connectivity","date":"1501007631","status":1,"details":"","description":"<div><span class=\"yellowfg\">11:34 AM PDT</span>&nbsp;We are investigating an issue with an external provider outside of our network, which may be impacting Internet connectivity between some customer networks and the SA-EAST-1 Region. Connectivity to instances and services within the Region is not impacted by the event.</div><div><span class=\"yellowfg\">12:17 PM PDT</span>&nbsp;Between 10:34 AM and 11:41 AM PDT we experienced elevated packet loss to and from some destinations on the Internet in the SA-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>","service":"internetconnectivity-sa-east-1"},{"service_name":"AWS Internet Connectivity (Ireland)","summary":"[RESOLVED] Internet Connectivity ","date":"1501010666","status":1,"details":"","description":"<div><span class=\"yellowfg\">12:24 PM PDT</span>&nbsp;Between 10:59 AM and 11:46 AM PDT we experienced elevated packet loss to some destinations on the Internet in the EU-WEST-1 Region. The issue has been resolved and the service is operating normally.</div>","service":"internetconnectivity-eu-west-1"},{"service_name":"Amazon Elastic Compute Cloud (N. Virginia)","summary":"[RESOLVED] Network Connectivity","date":"1501165681","status":1,"details":"","description":"<div><span class=\"yellowfg\"> 7:28 AM PDT</span>&nbsp;Between 6:47 AM and 7:10 AM PDT we experienced increased launch failures for EC2 Instances, degraded EBS volume performance and connectivity issues for some instances in a single Availability Zone in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>","service":"ec2-us-east-1"},{"service_name":"Amazon Elastic Compute Cloud (N. Virginia)","summary":"[RESOLVED] Network Connectivity","date":"1501716971","status":1,"details":"","description":"<div><span class=\"yellowfg\"> 4:36 PM PDT</span>&nbsp;We are investigating network connectivity issues for some instances in a single Availability Zone in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 4:58 PM PDT</span>&nbsp;We can confirm that some instances are unreachable and some EBS volumes are experiencing degraded performance in a single Availability Zone in the US-EAST-1 Region. Engineers are engaged and we are working to resolve the issue.</div><div><span class=\"yellowfg\"> 5:05 PM PDT</span>&nbsp;We have identified the root cause and are beginning to see recovery for instances and EBS volumes in the affected Availability Zone in the US-EAST-1 Region. We continue to work toward full resolution.</div><div><span class=\"yellowfg\"> 5:23 PM PDT</span>&nbsp;Between 4:24 PM and 5:09 PM PDT some instances were unreachable and some EBS volumes experienced degraded performance in a single Availability Zone in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>","service":"ec2-us-east-1"},{"service_name":"Amazon Relational Database Service (N. Virginia)","summary":"[RESOLVED] Network Connectivity","date":"1501718245","status":1,"details":"","description":"<div><span class=\"yellowfg\"> 4:57 PM PDT</span>&nbsp;We can confirm that some instances are unreachable in a single Availability Zone in the US-EAST-1 Region. We are working to resolve the issue.</div><div><span class=\"yellowfg\"> 5:24 PM PDT</span>&nbsp;We can confirm that most instances that were unreachable in a single Availability Zone in the US-EAST-1 Region are now reachable. We are working to restore connectivity to the remaining impacted instances.</div><div><span class=\"yellowfg\"> 5:37 PM PDT</span>&nbsp;Between 4:24 PM and 5:31 PM PDT, some instances were unreachable in a single Availability Zone in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>","service":"rds-us-east-1"},{"service_name":"Amazon Redshift (N. Virginia)","summary":"[RESOLVED] Network Connectivity","date":"1501718490","status":1,"details":"","description":"<div><span class=\"yellowfg\"> 5:01 PM PDT</span>&nbsp;We can confirm that some Redshift clusters are unreachable in a single Availability Zone in the US-EAST-1 Region. We are working to resolve the issue.</div><div><span class=\"yellowfg\"> 5:21 PM PDT</span>&nbsp;Between 4:24 PM and 4:58 PM PDT some Redshift clusters were unreachable in a single Availability Zone in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>","service":"redshift-us-east-1"},{"service_name":"Amazon Elastic Compute Cloud (N. California)","summary":"[RESOLVED] Increased Launch Error Rates","date":"1501891033","status":1,"details":"","description":"<div><span class=\"yellowfg\"> 4:57 PM PDT</span>&nbsp;We are investigating increased error rates for new instance launches in a single Availability Zone in the US-WEST-1 Region.</div><div><span class=\"yellowfg\"> 5:33 PM PDT</span>&nbsp;We continue to work towards resolving the increased errors for new instance launches in a single Availability Zone in the US-WEST-1 Region. We have identified a sub-system that is experiencing increased error rates within the affected Availability Zone and are working towards root cause.</div><div><span class=\"yellowfg\"> 6:53 PM PDT</span>&nbsp;Between 4:32 PM and 6:22 PM PDT we experienced increased errors for new instance launches in a single Availability Zone in the US-WEST-1 Region. The issue has been resolved and the service is operating normally.</div>","service":"ec2-us-west-1"},{"service_name":"Amazon Chime","summary":"[RESOLVED] Increased Meeting Launch Error Rates","date":"1502160714","status":1,"details":"","description":"<div><span class=\"yellowfg\"> 8:03 PM PDT</span>&nbsp;Between 6:24 PM and 7:48 PM PDT we experienced increased meeting start and join error rates. Messaging services continued to operate normally during this time. The issue has been resolved and the service is operating normally.</div>","service":"chime"},{"service_name":"AWS Config (Oregon)","summary":"[RESOLVED] Increased Latency","date":"1502298882","status":1,"details":"","description":"<div><span class=\"yellowfg\">10:14 AM PDT</span>&nbsp;We are investigating elevated latencies in the processing of configuration changes and evaluation of Config Rules in the US-WEST-2 Region.</div><div><span class=\"yellowfg\">10:49 AM PDT</span>&nbsp;We continue to investigate elevated latencies in the processing of configuration changes and evaluation of Config Rules in the US-WEST-2 Region.</div><div><span class=\"yellowfg\">11:36 AM PDT</span>&nbsp;We can confirm elevated latencies in the processing of configuration changes and evaluation of Config Rules in the US-WEST-2 Region and continue to work towards resolution.</div><div><span class=\"yellowfg\">12:53 PM PDT</span>&nbsp;We are beginning to see recovery of elevated latencies in the processing of configuration changes and evaluation of Config Rules in the US-WEST-2 Region. We are currently working on processing the delayed changes and evaluations and continue to work towards full recovery. </div><div><span class=\"yellowfg\"> 1:19 PM PDT</span>&nbsp;Configuration changes and evaluations are now up to date. The issue has been resolved and the service is operating normally.</div>","service":"config-us-west-2"},{"service_name":"AWS Elastic Beanstalk (N. Virginia)","summary":"[RESOLVED] Increased Latencies Creating, Updating and Terminating Environments","date":"1502371879","status":1,"details":"","description":"<div><span class=\"yellowfg\"> 6:31 AM PDT</span>&nbsp;We are investigating increased latencies and timeouts when creating, updating and terminating Elastic Beanstalk environments in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 6:52 AM PDT</span>&nbsp;We continue to investigate increased latencies and timeouts when creating, updating and terminating Elastic Beanstalk environments in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 7:50 AM PDT</span>&nbsp;Between 3:15 AM and 7:30 AM PDT some customers experienced elevated latencies or timeouts when creating, updating, and terminating Elastic Beanstalk environments in the US-EAST-1 Region. Already running environments were not affected at any time. The issue has been resolved and the service is operating normally.</div>","service":"elasticbeanstalk-us-east-1"},{"service_name":"Amazon WorkDocs (Oregon)","summary":"[RESOLVED] Increased Error Rates","date":"1502691286","status":1,"details":"","description":"<span style=\"font-size:small;color:#006666\">\r\n  <h4>[RESOLVED] Increased Error Rates </h4>\r\n  <div><span class=\"yellowfg\">12:30 PM PDT</span> We can confirm increased error rates when accessing WorkDocs impacting some customers in the US-WEST-2 Region.</div>\r\n   <div><span class=\"yellowfg\">1:48 PM PDT</span> Between 10:54 AM and 1:35 PM PDT, we experienced increased error rates when accessing WorkDocs that impacted some customers in the US-WEST-2 Region. The issue has been resolved and the service is operating normally.</div>\r\n  <br>\r\n  <h4>[RESOLVED] Increased Latency</h4>\r\n <div><span class=\"yellowfg\">Aug 13, 11:14 PM PDT</span>&nbsp;We are investigating increased API latencies and degraded application performance in the US-WEST-2 Region.</div>\r\n <div><span class=\"yellowfg\">Aug 14, 12:04 AM PDT</span>&nbsp;We continue to investigate increased API latencies and degraded application performance in the US-WEST-2 Region.</div>\r\n <div><span class=\"yellowfg\">Aug 14,  2:12 AM PDT</span>&nbsp;We are still investigating increased API latencies and degraded application performance for a small number of customers in the US-WEST-2 Region.</div>\r\n <div><span class=\"yellowfg\">Aug 14,  3:06 AM PDT</span>&nbsp;Between August 13 8:33 PM and August 14 2:24 AM PDT we experienced increased API latencies and degraded application performance affecting a small number of customers in the US-WEST-2 Region. The issue has been resolved and the service is operating normally. </div>\r\n</span>","service":"workdocs-us-west-2"},{"service_name":"AWS Identity and Access Management (N. Virginia)","summary":"[RESOLVED] Increased Error Rates","date":"1502823125","status":2,"details":"","description":"<div><span class=\"yellowfg\">11:52 AM PDT</span>&nbsp;We are investigating increased error rates accessing the IAM Management Console and IAM service.</div><div><span class=\"yellowfg\">12:36 PM PDT</span>&nbsp;We continue to investigate increased error rates and latencies when accessing the IAM Management Console and IAM APIs. Creation and listing of new IAM users, groups and roles are experiencing increased error rates, latencies and propagation times. Authentication for existing IAM users, groups and roles is not affected.</div><div><span class=\"yellowfg\">12:50 PM PDT</span>&nbsp;We have identified the root cause and taken steps to resolve the issue. Error rates and latencies have improved and we continue to work towards full recovery.</div><div><span class=\"yellowfg\"> 1:36 PM PDT</span>&nbsp;We have resolved the increased error rates and latencies for the IAM Management Console and IAM API. Authentication and authorization of existing IAM users, groups, and roles were not affected. The issue has been resolved and the service is operating normally.</div>","service":"iam-us-east-1"},{"service_name":"Amazon Elastic Compute Cloud (Ireland)","summary":"[RESOLVED] Instance Connectivity","date":"1503007374","status":1,"details":"","description":"<div><span class=\"yellowfg\"> 3:03 PM PDT</span>&nbsp;We are investigating network connectivity issues for some instances in a single Availability Zone in the EU-WEST-1 Region.</div><div><span class=\"yellowfg\"> 3:31 PM PDT</span>&nbsp;We have identified the root cause of the network connectivity issue affecting some instances in a single Availability Zone in the EU-WEST-1 Region and continue to work on restoring network connectivity for the affected instances. Launches on new EC2 instances are not affected.</div><div><span class=\"yellowfg\"> 3:39 PM PDT</span>&nbsp;Network connectivity has been restored to some of the affected instances. We continue to work towards full recovery.</div><div><span class=\"yellowfg\"> 4:07 PM PDT</span>&nbsp;Between 2:28 PM and 3:57 PM PDT we experienced network connectivity issues for some instances in a single Availability Zone in the EU-WEST-1 Region. The issue has been resolved and the service is operating normally.</div>","service":"ec2-eu-west-1"},{"service_name":"Amazon Elastic Compute Cloud (Ireland)","summary":"[RESOLVED] Network connectivity","date":"1503180564","status":1,"details":"","description":"<div><span class=\"yellowfg\"> 3:09 PM PDT</span>&nbsp;Between 2:37 PM and 2:55 PM PDT we experienced network connectivity issues to the Internet and other AWS services for some instances in a single Availability Zone in the EU-WEST-1 Region.</div>","service":"ec2-eu-west-1"},{"service_name":"Amazon Elastic Compute Cloud (Ireland)","summary":"[RESOLVED] Network Connectivity","date":"1503513149","status":1,"details":"","description":"<div><span class=\"yellowfg\">11:32 AM PDT</span>&nbsp;We are investigating network connectivity issues for some instances in the EU-WEST-1 Region.</div><div><span class=\"yellowfg\">11:55 AM PDT</span>&nbsp;We have identified root cause of the network connectivity issues in the EU-WEST-1 Region. Connectivity between peered VPCs is affected by this issue. Connectivity between instances within a VPC or between instances and the Internet or AWS services is not affected. We continue to work towards full recovery.</div><div><span class=\"yellowfg\">12:51 PM PDT</span>&nbsp;Between 10:32 AM and 12:44 PM PDT we experienced connectivity issues when using VPC peering in the EU-WEST-1 Region. Connectivity between instances in the same VPC and from instances to the Internet or AWS services was not affected. The issue has been resolved and the service is operating normally.</div>","service":"ec2-eu-west-1"},{"service_name":"AWS Lambda (Ireland)","summary":"[RESOLVED] Increased API Error Rates","date":"1503513333","status":1,"details":"","description":"<div><span class=\"yellowfg\">11:35 AM PDT</span>&nbsp;We are investigating increased API error rates and event processing latency in the EU-WEST-1 Region. </div><div><span class=\"yellowfg\">12:22 PM PDT</span>&nbsp;Between 10:35 AM and 12:15 PM PDT we experienced elevated API error rates and event processing latencies in the EU-WEST-1 Region. The issue has been resolved and the service is operating normally. </div>","service":"lambda-eu-west-1"},{"service_name":"Amazon API Gateway (Ireland)","summary":"[RESOLVED] Increased Error Rates for AWS Lambda integrations","date":"1503514206","status":1,"details":"","description":"<div><span class=\"yellowfg\">11:50 AM PDT</span>&nbsp;We are investigating increased error rates for AWS Lambda integrations in the EU-WEST-1 Region.</div><div><span class=\"yellowfg\">12:22 PM PDT</span>&nbsp;Between 10:35 AM and 12:15 PM PDT we experienced elevated error rates for Lambda function integrations in the EU-WEST-1 Region. The issue has been resolved and the service is operating normally.</div>","service":"apigateway-eu-west-1"},{"service_name":"AWS Batch (Ireland)","summary":"[RESOLVED] Increased Error Rates","date":"1503514297","status":1,"details":"","description":"<div><span class=\"yellowfg\">11:51 AM PDT</span>&nbsp;We can confirm elevated error rates for AWS Batch in the EU-WEST-1 Region.</div><div><span class=\"yellowfg\">12:23 PM PDT</span>&nbsp;Between 10:35 AM and 11:59 AM PDT we experienced elevated error rates in the EU-WEST-1 Region. The issue has been resolved and the service is operating normally.</div>","service":"batch-eu-west-1"},{"service_name":"Amazon Route 53","summary":"[RESOLVED] Increased Propagation Time","date":"1505229685","status":1,"details":"","description":"<div><span class=\"yellowfg\"> 8:21 AM PDT</span>&nbsp;We are investigating increased propagation times of DNS edits to the Route 53 DNS servers. This does not impact queries to existing DNS records.</div><div><span class=\"yellowfg\"> 8:57 AM PDT</span>&nbsp;We can confirm increased propagation times of DNS edits to the Route 53 DNS servers and continue to work towards resolution. This does not impact queries to existing DNS records.</div><div><span class=\"yellowfg\"> 9:38 AM PDT</span>&nbsp;We have identified the root cause for the increased propagation times of DNS edits to the Route 53 DNS servers and continue to work towards resolution. This does not impact queries to existing DNS records. </div><div><span class=\"yellowfg\">10:14 AM PDT</span>&nbsp;Between 7:20 AM and 10:06 AM PDT, we experienced increased propagation times of DNS edits to the Route 53 DNS servers. This did not impact queries to existing DNS records. The issue has been resolved and the service is operating normally.</div>","service":"route53"},{"service_name":"Amazon Simple Storage Service (N. Virginia)","summary":"[RESOLVED] Increased Error Rates","date":"1505415474","status":2,"details":"","description":"<div><span class=\"yellowfg\">11:58 AM PDT</span>&nbsp;We are investigating increased error rates for Amazon S3 requests in the US-EAST-1 Region.</div><div><span class=\"yellowfg\">12:20 PM PDT</span>&nbsp;We can confirm that some customers are receiving throttling errors accessing S3. We are currently investigating the root cause.</div><div><span class=\"yellowfg\">12:38 PM PDT</span>&nbsp;We continue to work towards resolving the increased throttling errors for Amazon S3 requests in the US-EAST-1 Region. We have identified the subsystem responsible for the errors, identified root cause and are now working to resolve the issue.</div><div><span class=\"yellowfg\">12:49 PM PDT</span>&nbsp;We are now seeing recovery in the throttle error rates accessing Amazon S3. We have identified the root cause and have taken actions to prevent recurrence.</div><div><span class=\"yellowfg\"> 1:05 PM PDT</span>&nbsp;Between 11:40 AM and 12:56 PM PDT we experienced throttling errors accessing Amazon S3 in the US-EAST-1 Region. The issue is resolved and the service is operating normally.</div>","service":"s3-us-standard"},{"service_name":"AWS CodeCommit (N. Virginia)","summary":"[RESOLVED] Increased Error Rates","date":"1505417277","status":1,"details":"","description":"<div><span class=\"yellowfg\">12:28 PM PDT</span>&nbsp;We are investigating increased error rates for Git Push, Git Pull and API calls in the US-EAST-1 Region. </div><div><span class=\"yellowfg\">12:54 PM PDT</span>&nbsp;We are seeing a recovery for Git Push, Git Pull and API calls in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 1:07 PM PDT</span>&nbsp;Between 11:40 AM and 12:56 PM PDT we experienced increased error rates and latencies for the Git Pull, Git Push, and API calls in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>","service":"codecommit-us-east-1"},{"service_name":"AWS Storage Gateway (N. Virginia)","summary":"[RESOLVED] Increased Error Rates","date":"1505417505","status":1,"details":"","description":"<div><span class=\"yellowfg\">12:31 PM PDT</span>&nbsp;We are investigating increased error rates for Storage Gateway read/write operations in the US-EAST-1 Region.</div><div><span class=\"yellowfg\">12:53 PM PDT</span>&nbsp;We are seeing recovery for Storage Gateway read/write operations in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 1:10 PM PDT</span>&nbsp;Between 11:40 AM and 12:56 PM PDT we experienced increased latencies for read/write operations in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>","service":"storagegateway-us-east-1"},{"service_name":"AWS Elastic Beanstalk (N. Virginia)","summary":"[RESOLVED] Increased Error Rates","date":"1505417831","status":1,"details":"","description":"<div><span class=\"yellowfg\">12:37 PM PDT</span>&nbsp;We can confirm increased error rates and latencies for the Elastic Beanstalk APIs in the US-EAST-1 Region.</div><div><span class=\"yellowfg\">12:57 PM PDT</span>&nbsp;We are seeing recovery for Elastic Beanstalk API operations in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 1:08 PM PDT</span>&nbsp;Between 11:40 AM and 12:56 PM PDT we experienced increased API latencies and error rates for Elastic Beanstalk in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>","service":"elasticbeanstalk-us-east-1"},{"service_name":"Amazon Route 53","summary":"[RESOLVED] Resolution issues with .io domains","date":"1505920192","status":1,"details":"","description":"<div><span class=\"yellowfg\"> 8:10 AM PDT</span>&nbsp;Some customers have reported intermittent resolution issues with .io domains. We can confirm that Route 53 DNS services are operating normally at this time and these issues seem to be related to the .io top-level domain provider.</div><div><span class=\"yellowfg\"> 8:43 AM PDT</span>&nbsp;We can confirm that resolution of .io domain names are intermittently failing due to issues with the .io TLD name servers that are hosted external to AWS. Route 53 name servers continue to operate normally, but customers who have sub-domains of .io hosted on Route 53 may experience issues until the .io TLD name servers hosted externally are resolved.</div>","service":"route53"},{"service_name":"Amazon Elastic Compute Cloud (N. Virginia)","summary":"[RESOLVED]  Increased Launch Error Rates","date":"1507528068","status":1,"details":"","description":"<div><span class=\"yellowfg\">10:48 PM PDT</span>&nbsp;We are investigating increased error rates for new EBS-backed instance launches and EBS-related APIs in the US-EAST-1 Region.</div><div><span class=\"yellowfg\">11:32 PM PDT</span>&nbsp;We continue to investigate increased error rates for new EBS-backed instance launches and EBS-related APIs in the US-EAST-1 Region. Existing instances are unaffected.</div><div><span class=\"yellowfg\">11:51 PM PDT</span>&nbsp;Between 9:45 PM and 11:37 PM PDT we experienced increased error rates for EBS-backed instance launches and EBS-related APIs in the US-EAST-1 Region. Existing instances were unaffected. The issue has been resolved and the service is operating normally.</div>","service":"ec2-us-east-1"},{"service_name":"AWS Direct Connect (N. Virginia)","summary":"[RESOLVED] Inaccurate BGP Status","date":"1507753736","status":1,"details":"","description":"<div><span class=\"yellowfg\"> 1:29 PM PDT</span>&nbsp;Between 11:25 AM PDT and 11:50 AM PDT we incorrectly displayed the BGP status for virtual interfaces on Direct Connect connections in the US-EAST-1 Region. Despite the status, customers' virtual interfaces were not impaired and continued to send and receive traffic normally. The issue has been resolved and the service is operating normally.</div>","service":"directconnect-us-east-1"},{"service_name":"Amazon Simple Notification Service (Seoul)","summary":"[RESOLVED] Increased Notification Latencies","date":"1507907819","status":1,"details":"","description":"<div><span class=\"yellowfg\"> 8:17 AM PDT</span>&nbsp;We are investigating increased Notification latencies in the AP-NORTHEAST-2 Region</div><div><span class=\"yellowfg\"> 8:31 AM PDT</span>&nbsp;We have identified the cause for increased Notification latencies in the AP-NORTHEAST-2 Region and continue to work towards resolution. Notification latencies are improving.</div><div><span class=\"yellowfg\"> 9:04 AM PDT</span>&nbsp;We have identified the cause for increased Notification latencies in the AP-NORTHEAST-2 Region and continue to work towards resolution. Notification latencies continue to improve.</div><div><span class=\"yellowfg\"> 9:33 AM PDT</span>&nbsp;Between 7:15 AM and 8:53 AM PDT, we experienced elevated Notification latencies in the AP-NORTHEAST-2 Region. The issue has been resolved and the service is operating normally.</div>","service":"sns-ap-northeast-2"},{"service_name":"Amazon Elastic Compute Cloud (N. Virginia)","summary":"[RESOLVED] Network Connectivity","date":"1507934175","status":1,"details":"","description":"<div><span class=\"yellowfg\"> 3:36 PM PDT</span>&nbsp;We are investigating intermittent connectivity issues to a single Availability Zone in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 4:07 PM PDT</span>&nbsp;We can confirm connectivity issues to some instances in the US-EAST-1 Region. We have identified the root cause and are working to resolve. </div><div><span class=\"yellowfg\"> 5:18 PM PDT</span>&nbsp;We continue to work toward full recovery. For customers that are still seeing impact, you can perform a Stop/Start to resolve the issue.  More information on how to Stop/Start your instance can be found <a href=\"http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Stop_Start.html\">here.</a></div><div><span class=\"yellowfg\"> 5:49 PM PDT</span>&nbsp;Connectivity to most of the affected instances has been restored. We are continuing to work to remediate connectivity to all instances. </div><div><span class=\"yellowfg\"> 6:15 PM PDT</span>&nbsp;We have remediated the connectivity issues that were affecting some instances in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>","service":"ec2-us-east-1"},{"service_name":"Amazon Relational Database Service (N. Virginia)","summary":" [RESOLVED] Network Connectivity ","date":"1507941037","status":1,"details":"","description":"<div><span class=\"yellowfg\"> 5:30 PM PDT</span>&nbsp;Between 2:15 PM and 4:30 PM PDT we experienced connectivity issues in the US-EAST-1 Region. The service has recovered and is operating normally. </div>","service":"rds-us-east-1"},{"service_name":"AWS IoT (Ireland)","summary":"[RESOLVED] Increased API Errors and Latencies ","date":"1508245415","status":1,"details":"","description":"<div><span class=\"yellowfg\"> 6:03 AM PDT</span>&nbsp;Between 4:00 AM and 4:42 AM PDT we experienced increased API error rates and latencies in the EU-WEST-1 Region. The issue has been resolved and the service is operating normally.</div>","service":"awsiot-eu-west-1"},{"service_name":"AWS Internet Connectivity (Oregon)","summary":"[RESOLVED] Network Connectivity ","date":"1508360382","status":1,"details":"","description":"<div><span class=\"yellowfg\"> 1:59 PM PDT</span>&nbsp;We are investigating Network Connectivity issues in the US-WEST-2 Region. </div><div><span class=\"yellowfg\"> 2:33 PM PDT</span>&nbsp;Between 1:36 PM and 2:15 PM PDT we experienced network connectivity issues in the US-WEST-2 Region. The issue has been resolved and the service is operating normally. </div>","service":"internetconnectivity-us-west-2"},{"service_name":"AWS Internet Connectivity (GovCloud)","summary":"[RESOLVED] Network Connectivity ","date":"1508360877","status":1,"details":"","description":"<div><span class=\"yellowfg\"> 2:08 PM PDT</span>&nbsp;We are investigating Network Connectivity issues in the US-GOV-WEST-1 Region.</div><div><span class=\"yellowfg\"> 2:25 PM PDT</span>&nbsp;We have identified the root cause of the network connectivity issues in the US-GOV-WEST-1 Region. We are seeing signs of recovery but continue to work towards fully resolving the issue.</div><div><span class=\"yellowfg\"> 2:32 PM PDT</span>&nbsp;Between 1:36 PM and 2:15 PM PDT we experienced network connectivity issues in the US-GOV-WEST-1 Region. The issue has been resolved and the service is operating normally.</div>","service":"internetconnectivity-us-gov-west-1"},{"service_name":"AWS Direct Connect (Oregon)","summary":"[RESOLVED] Network Connectivity","date":"1508361019","status":1,"details":"","description":"<div><span class=\"yellowfg\"> 2:10 PM PDT</span>&nbsp;We are investigating network connectivity issues affecting Direct Connect customers using the US-WEST-2 Region. </div><div><span class=\"yellowfg\"> 2:37 PM PDT</span>&nbsp;Between 1:36 PM and 2:15 PM PDT we experienced network connectivity issues affecting some Direct Connect locations using the US-WEST-2 Region. The issue has been resolved and the service is operating normally.</div>","service":"directconnect-us-west-2"},{"service_name":"AWS Direct Connect (GovCloud)","summary":"[RESOLVED] Network Connectivity","date":"1508361260","status":1,"details":"","description":"<div><span class=\"yellowfg\"> 2:14 PM PDT</span>&nbsp;We are investigating network connectivity issues affecting Direct Connect customers using the US-GOV-WEST-1 Region. </div><div><span class=\"yellowfg\"> 2:35 PM PDT</span>&nbsp;Between 1:36 PM and 2:15 PM PDT we experienced network connectivity issues affecting Direct Connect customers using the US-GOV-WEST-1 Region. The issue has been resolved and the service is operating normally.</div>","service":"directconnect-us-gov-west-1"}],"current":[{"service_name":"Amazon DynamoDB (N. Virginia)","summary":"[RESOLVED] Increased API Error Rates","date":"1508782371","status":"0","details":"","description":"<div><span class=\"yellowfg\">11:13 AM PDT</span>&nbsp;We're investigating increased error rates for DynamoDB APIs in the US-EAST-1 Region. </div><div><span class=\"yellowfg\">11:34 AM PDT</span>&nbsp;Between 10:10 AM and 11:23 AM PDT we experienced increased error rates for DynamoDB APIs in the US-EAST-1 Region. The issue is resolved and the service is operating normally. </div>","service":"dynamodb-us-east-1"},{"service_name":"Amazon WorkMail (N. Virginia)","summary":"[RESOLVED] Increased Latencies","date":"1508782622","status":"0","details":"","description":"<div><span class=\"yellowfg\">11:17 AM PDT</span>&nbsp;We are investigating increased Workmail Outlook latencies in the US-EAST-1 Region impacting the sending and receiving of new messages or appointments in Outlook clients.</div><div><span class=\"yellowfg\">11:33 AM PDT</span>&nbsp;Between 10:16 AM and 11:07 AM PDT we experienced degraded WorkMail Outlook performances in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>","service":"workmail-us-east-1"}]}